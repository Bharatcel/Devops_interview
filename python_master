# Chapter 18: Python Masterclass - From Fundamentals to Advanced Engineering

## Abstract

Python's simplicity, readability, and vast ecosystem have made it one of the most dominant programming languages in the world, powering everything from web servers and data science models to infrastructure automation and machine learning. For a DevOps/SRE engineer, a deep understanding of Python is not just a "nice-to-have"; it is a fundamental requirement for writing automation scripts, building internal tools, and interacting with cloud provider SDKs. This chapter provides a definitive, book-level guide to Python, designed for engineers. We will move from the foundational syntax to advanced concepts like concurrency, decorators, and the data model, and conclude with practical interview scenarios that test your ability to solve real-world problems with idiomatic Python.

---

### Part 1: Python Fundamentals - The Bedrock

#### 1.1 The Zen of Python & Environment Setup

*   **The Zen of Python (`import this`):** Python's design philosophy is captured in 19 guiding principles, including "Beautiful is better than ugly," "Simple is better than complex," and "Readability counts." This emphasis on clarity is a core feature of the language.
*   **Interpreter:** Python is an interpreted language. The CPython interpreter is the standard implementation.
*   **Virtual Environments (`venv`):** You should **never** use the system's global Python for project development. A virtual environment is an isolated Python environment that allows you to manage dependencies for a specific project independently.
    *   **Creation:** `python -m venv venv`
    *   **Activation:** `source venv/bin/activate` (Linux/macOS) or `.\venv\Scripts\activate` (Windows)
*   **Package Management (`pip`):** `pip` is Python's package installer. You use it to install libraries from the Python Package Index (PyPI).
    *   **Installation:** `pip install requests`
    *   **Dependency Freezing:** `pip freeze > requirements.txt`
    *   **Installing from file:** `pip install -r requirements.txt`

#### 1.2 Core Data Types and Structures

*   **Primitive Types:**
    *   **Numbers:** `int`, `float`.
    *   **Strings:** Immutable sequences of characters. `f-strings` (e.g., `f"Hello, {name}"`) are the modern, preferred way to format strings.
    *   **Booleans:** `True`, `False`.
*   **Data Structures (The "Collections"):**
    *   **List:** A mutable, ordered sequence of elements. `my_list = [1, "a", 3.0]`
        *   *Use Case:* When you need an ordered collection that you can modify.
    *   **Tuple:** An immutable, ordered sequence of elements. `my_tuple = (1, "a", 3.0)`
        *   *Use Case:* When you need an ordered collection that should not be changed (e.g., returning multiple values from a function, dictionary keys).
    *   **Dictionary (`dict`):** A mutable collection of key-value pairs. Keys must be immutable types (like strings or tuples). `my_dict = {"name": "Alice", "age": 30}`
        *   *Use Case:* Storing and retrieving data by a unique key. The fundamental mapping type in Python.
    *   **Set:** A mutable, unordered collection of *unique* elements. `my_set = {1, 2, 3}`
        *   *Use Case:* Checking for membership (`in`) or removing duplicates from a list.

#### 1.3 Control Flow & Functions

*   **Conditional Logic:** `if`, `elif`, `else`.
*   **Loops:**
    *   `for` loop: Iterates over a sequence (e.g., a list, a string, or a `range`).
    *   `while` loop: Executes as long as a condition is true.
*   **Functions:**
    *   Defined with the `def` keyword.
    *   **Arguments:** Can have positional arguments, keyword arguments (with default values).
    *   **`*args` and `**kwargs`:** A powerful feature for writing functions that accept a variable number of arguments.
        *   `*args`: Collects extra positional arguments into a tuple.
        *   `**kwargs`: Collects extra keyword arguments into a dictionary.
        ```python
        def my_function(a, b, *args, **kwargs):
            print(f"a={a}, b={b}")
            print(f"args={args}") # e.g., (3, 4)
            print(f"kwargs={kwargs}") # e.g., {'c': 5, 'd': 6}

        my_function(1, 2, 3, 4, c=5, d=6)
        ```
    *   **Scope:** Python has LEGB scope resolution: **L**ocal, **E**nclosing, **G**lobal, **B**uilt-in.

#### 1.4 Error Handling

*   The `try...except` block is used to handle exceptions gracefully.
    ```python
    try:
        result = 10 / 0
    except ZeroDivisionError as e:
        print(f"Error: Cannot divide by zero. Details: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
    finally:
        print("This block always executes, for cleanup.")
    ```

---

### Part 2: Intermediate Python & Software Engineering

#### 2.1 Object-Oriented Programming (OOP)

*   **Classes and Objects:** A class is a blueprint for creating objects. An object is an instance of a class.
*   **The `__init__` method:** The constructor for a class. It's called when a new object is created. `self` refers to the instance of the object being created.
*   **The Four Pillars of OOP:**
    1.  **Encapsulation:** Bundling data (attributes) and methods that operate on the data into a single unit (a class). Using "private" attributes (by convention, prefixed with an underscore, e.g., `_internal_var`) hides implementation details.
    2.  **Inheritance:** Creating a new class (child class) from an existing class (parent class). The child class inherits the attributes and methods of the parent.
    3.  **Polymorphism:** The ability of different classes to be treated as instances of a common superclass. For example, if `Dog` and `Cat` both inherit from `Animal` and both have a `speak()` method, you can call `animal.speak()` without knowing if it's a dog or a cat.
    4.  **Abstraction:** Hiding complex implementation details and exposing only the necessary features of an object.

#### 2.2 Modules and Packages

*   **Module:** Any `.py` file is a module.
*   **Package:** A directory containing modules. It **must** contain a file named `__init__.py` (which can be empty) to be considered a package. This allows you to structure your code logically.
    ```
    my_project/
    ├── main.py
    └── my_package/
        ├── __init__.py
        ├── module1.py
        └── module2.py
    ```
    In `main.py`, you can then do `from my_package import module1`.

#### 2.3 Comprehensions & Generators

*   **Comprehensions:** A concise, readable way to create lists, dicts, or sets.
    *   **List Comprehension:** `squares = [x**2 for x in range(10)]`
    *   **Dict Comprehension:** `square_dict = {x: x**2 for x in range(10)}`
*   **Generators:** Generators look like functions but use the `yield` keyword to return data. They produce items one at a time and only when requested. This makes them incredibly memory-efficient for working with large data sets.
    *   **Generator Function:**
        ```python
        def count_up_to(max):
            count = 1
            while count <= max:
                yield count
                count += 1
        ```
    *   **Generator Expression:** Similar to a list comprehension, but with parentheses. It creates a generator object without building the full list in memory.
        `lazy_squares = (x**2 for x in range(1000000))`

#### 2.4 Decorators

*   A decorator is a function that takes another function as an argument, adds some functionality, and returns another function, all without altering the source code of the original function.
*   *Use Case:* Logging, timing, authentication checks.
    ```python
    import time

    def timing_decorator(func):
        def wrapper(*args, **kwargs):
            start_time = time.time()
            result = func(*args, **kwargs)
            end_time = time.time()
            print(f"Function {func.__name__} took {end_time - start_time:.4f} seconds")
            return result
        return wrapper

    @timing_decorator
    def slow_function():
        time.sleep(2)

    slow_function() # Will print the execution time
    ```

---

### Part 3: Advanced Python

#### 3.1 Concurrency Models

*   **Threading:** Uses threads to run tasks concurrently. In CPython, due to the **Global Interpreter Lock (GIL)**, only one thread can execute Python bytecode at a time. This means threading is great for **I/O-bound** tasks (like making network requests), where threads can wait for I/O without blocking each other, but it does not provide true parallelism for **CPU-bound** tasks.
*   **Multiprocessing:** Bypasses the GIL by creating separate processes, each with its own Python interpreter and memory space. This allows for true parallel execution on multi-core CPUs. It's ideal for **CPU-bound** tasks (like data processing or calculations).
*   **Asyncio:** A single-threaded, single-process approach to concurrency that uses an **event loop**. It's designed for a large number of I/O-bound tasks. Using `async` and `await` syntax, you can write code that "pauses" when it's waiting for I/O and allows the event loop to run other tasks. It can be much more efficient than threading for a very high number of concurrent connections.

#### 3.2 Context Managers (`with` statement)

*   The `with` statement simplifies resource management by ensuring that cleanup logic (like closing a file or a network connection) is always executed. Any object that has `__enter__` and `__exit__` methods can be used as a context manager.
    ```python
    # The right way to open a file
    with open("my_file.txt", "w") as f:
        f.write("Hello, world!")
    # The file is automatically closed here, even if errors occur inside the block.
    ```

#### 3.3 The Python Data Model (Dunder Methods)

*   "Dunder" (double underscore) methods like `__init__`, `__str__`, `__len__`, and `__add__` allow your custom objects to integrate with Python's built-in syntax and functions. Implementing `__len__` lets you call `len(my_object)`. Implementing `__add__` lets you use the `+` operator on your objects. This is the key to making your classes feel "Pythonic."

---

### Part 4: The Python Ecosystem

*   **Web Development:**
    *   **Flask:** A lightweight "micro-framework" that is excellent for building APIs and smaller web applications.
    *   **Django:** A powerful, "batteries-included" framework that provides an ORM, admin interface, and everything you need for large, complex web applications.
*   **Data Science & Machine Learning:**
    *   **NumPy:** The fundamental package for numerical computing, providing a powerful N-dimensional array object.
    *   **Pandas:** Provides high-performance, easy-to-use data structures (like the DataFrame) and data analysis tools.
    *   **Scikit-learn:** A comprehensive library of machine learning algorithms.
*   **Testing:**
    *   **pytest:** The de facto standard for testing in Python. It has a simple, clean syntax for writing tests and a powerful plugin ecosystem.

---

### Part 5: Practice Questions & Scenarios

#### Q1 (Basic): What will be the output of `print(my_list)`?
```python
my_list = [1, 2, 3]
my_other_list = my_list
my_other_list.append(4)
print(my_list)
```
**Answer:** The output will be `[1, 2, 3, 4]`. This is because lists are mutable objects. `my_other_list = my_list` does not create a copy of the list; it makes both variables point to the *same* list object in memory. To create a shallow copy, you would use `my_other_list = my_list.copy()` or `my_other_list = my_list[:]`.

#### Q2 (Intermediate): Write a list comprehension to create a list of all even numbers from 0 to 20.
**Answer:**
```python
evens = [x for x in range(21) if x % 2 == 0]
```

#### Q3 (Intermediate): What is the GIL and how does it affect Python performance?
**Answer:** The GIL, or Global Interpreter Lock, is a mutex (a lock) in the CPython interpreter that protects access to Python objects, preventing multiple threads from executing Python bytecode at the same time. The consequence is that even on a multi-core processor, a multi-threaded Python program can only use one core at a time for executing Python code. This makes threading unsuitable for speeding up CPU-bound tasks. However, it is still very effective for I/O-bound tasks, because the GIL is released when a thread is waiting for I/O (like a network response), allowing another thread to run. To achieve true CPU parallelism, you must use the `multiprocessing` module.

#### Q4 (Advanced/Scenario): You are writing a script to process a 10 GB log file and count the number of lines containing the word "ERROR". Your first attempt is this:
```python
def count_errors(filename):
    with open(filename, 'r') as f:
        lines = f.readlines()
    error_count = 0
    for line in lines:
        if "ERROR" in line:
            error_count += 1
    return error_count
```
**Why is this a bad approach for a large file, and how would you rewrite it to be memory-efficient?**
**Answer:** This is a bad approach because `f.readlines()` reads the *entire* 10 GB file into memory at once, which will likely crash the machine or cause it to swap heavily.

The memory-efficient, "Pythonic" way to solve this is to iterate over the file object directly. The file object acts as a generator, reading the file line-by-line without loading it all into memory.

**First Improvement (Line-by-line iteration):**
```python
def count_errors_v2(filename):
    error_count = 0
    with open(filename, 'r') as f:
        for line in f: # This iterates line-by-line
            if "ERROR" in line:
                error_count += 1
    return error_count
```

**Second Improvement (Using a generator expression and `sum`):**
This is an even more concise and idiomatic solution. It uses a generator expression to create a sequence of `1`s for each error line, and then `sum` adds them up. This is still fully memory-efficient.
```python
def count_errors_v3(filename):
    with open(filename, 'r') as f:
        return sum(1 for line in f if "ERROR" in line)
```

#### Q5 (Advanced/Scenario): You need to write a function that fetches data from a list of 100 different URLs. The naive approach is to loop through the URLs and fetch them one by one, but this is very slow. How would you use Python's concurrency features to speed this up significantly? Provide code examples for two different approaches.
**Answer:** This is a classic I/O-bound problem, perfect for concurrency. A sequential approach is slow because the program spends most of its time waiting for network responses.

**Approach 1: Using `concurrent.futures` with Threading (Good for simplicity)**
The `concurrent.futures` module provides a high-level interface for asynchronously executing callables. `ThreadPoolExecutor` is ideal for this I/O-bound task.

```python
import concurrent.futures
import requests

URLS = ["http://example.com"] * 100 # A list of 100 URLs

def fetch_url(url):
    try:
        response = requests.get(url, timeout=10)
        return response.status_code
    except requests.RequestException as e:
        return str(e)

# Use a with statement to ensure threads are cleaned up
with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
    # map() runs the function for each item in the iterable
    results = executor.map(fetch_url, URLS)

for url, result in zip(URLS, results):
    print(f"URL: {url}, Result: {result}")
```
This is much faster because up to 20 requests are happening concurrently. While one thread is waiting for a response from a server, another thread can be sending a request to a different server.

**Approach 2: Using `asyncio` and `aiohttp` (Best for very high concurrency)**
For a very large number of URLs (thousands or more), `asyncio` can be even more efficient than threading as it has less overhead per task. It requires using an async-compatible HTTP library like `aiohttp`.

```python
import asyncio
import aiohttp

URLS = ["http://example.com"] * 100

async def fetch_url_async(session, url):
    try:
        async with session.get(url, timeout=10) as response:
            return await response.read() # Just get the content
    except Exception as e:
        return str(e)

async def main():
    # Create a single session for all requests
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url_async(session, url) for url in URLS]
        # asyncio.gather runs all tasks concurrently
        results = await asyncio.gather(*tasks)
        for url, result in zip(URLS, results):
            # Printing just the length to avoid huge output
            print(f"URL: {url}, Result length: {len(result)}")

# Run the main async function
asyncio.run(main())
```
This approach uses a single thread and an event loop to manage all 100 requests concurrently. It's often the highest-performance solution for I/O-bound problems in Python.

---

### Part 6: Python for DevOps - Scripting Interview Questions

This section focuses on practical, hands-on scripting problems that are representative of what a DevOps or SRE engineer does day-to-day. These questions test your ability to use Python's standard library and common third-party packages to automate real-world tasks.

#### Scenario 1 (Basic): Find and Archive Old Files

*   **Problem:** Write a script that scans a given directory for files ending with `.log`. Any log file older than 14 days should be compressed with `gzip` and moved to an `archive` subdirectory. The script should create the `archive` directory if it doesn't exist.

*   **Concepts Tested:** File system navigation (`os`, `pathlib`), date and time calculations (`datetime`), file manipulation (`shutil`), and running external processes (`subprocess`).

*   **Solution:**
    ```python
    import os
    import sys
    import shutil
    import subprocess
    from datetime import datetime, timedelta

    def archive_old_logs(directory_path, days_old=14):
        """
        Scans a directory for .log files, compresses and archives those older
        than a specified number of days.
        """
        # Ensure the target directory is valid
        if not os.path.isdir(directory_path):
            print(f"Error: Directory not found at '{directory_path}'")
            sys.exit(1)

        # Create the archive directory if it doesn't exist
        archive_dir = os.path.join(directory_path, "archive")
        os.makedirs(archive_dir, exist_ok=True)

        # Calculate the cutoff time
        cutoff_date = datetime.now() - timedelta(days=days_old)

        print(f"Scanning '{directory_path}' for log files older than {cutoff_date}...")

        # Walk through the directory
        for filename in os.listdir(directory_path):
            if filename.endswith(".log"):
                file_path = os.path.join(directory_path, filename)
                
                # Get file modification time
                file_mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))

                if file_mod_time < cutoff_date:
                    print(f"  -> Archiving '{filename}'...")
                    
                    # Compress the file using gzip
                    try:
                        subprocess.run(["gzip", file_path], check=True)
                        
                        # Move the compressed file to the archive directory
                        gzipped_filename = f"{filename}.gz"
                        gzipped_filepath = os.path.join(directory_path, gzipped_filename)
                        shutil.move(gzipped_filepath, archive_dir)
                        
                        print(f"     '{gzipped_filename}' moved to '{archive_dir}'")
                    
                    except subprocess.CalledProcessError as e:
                        print(f"     Error compressing '{filename}': {e}")
                    except FileNotFoundError:
                        print(f"     Error: 'gzip' command not found. Is it installed?")
                    except Exception as e:
                        print(f"     An unexpected error occurred: {e}")

    if __name__ == "__main__":
        # Example usage: python your_script.py /path/to/logs
        if len(sys.argv) != 2:
            print("Usage: python your_script.py <directory_path>")
            sys.exit(1)
        
        target_directory = sys.argv[1]
        archive_old_logs(target_directory)
    ```

#### Scenario 2 (Intermediate): Check Website Health and Report Status

*   **Problem:** Write a script that reads a list of URLs from a `urls.txt` file. For each URL, it should make an HTTP GET request. If the status code is `200 OK`, it should log that the site is "UP". Otherwise, it should log that the site is "DOWN" along with the status code. The script should handle network errors gracefully.

*   **Concepts Tested:** File I/O, making HTTP requests (`requests` library), error handling, and basic data processing.

*   **Solution:**
    ```python
    import requests
    import sys

    def check_website_health(filepath):
        """
        Reads URLs from a file and checks their HTTP status.
        """
        try:
            with open(filepath, 'r') as f:
                urls = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"Error: The file '{filepath}' was not found.")
            sys.exit(1)

        if not urls:
            print("The file is empty. No URLs to check.")
            return

        print("--- Website Health Check ---")
        for url in urls:
            try:
                # Make a request with a timeout to prevent hanging
                response = requests.get(url, timeout=5)
                
                if response.status_code == 200:
                    print(f"[ UP ] {url}")
                else:
                    print(f"[DOWN] {url} (Status Code: {response.status_code})")

            except requests.exceptions.Timeout:
                print(f"[DOWN] {url} (Error: Request timed out)")
            except requests.exceptions.ConnectionError:
                print(f"[DOWN] {url} (Error: Connection failed)")
            except requests.exceptions.RequestException as e:
                print(f"[DOWN] {url} (Error: {e})")
        print("--- Check Complete ---")


    if __name__ == "__main__":
        # Create a dummy urls.txt for testing
        with open("urls.txt", "w") as f:
            f.write("https://www.google.com\n")
            f.write("https://www.github.com\n")
            f.write("http://httpbin.org/status/404\n")
            f.write("https://invalid-domain-that-does-not-exist.com\n")

        check_website_health("urls.txt")
    ```

#### Scenario 3 (Advanced): Manage AWS S3 Buckets

*   **Problem:** Write a command-line tool using Python's `argparse` and `boto3` (the AWS SDK for Python) to perform basic S3 operations. The tool should support three actions:
    1.  `list`: List all S3 buckets.
    2.  `list-files <bucket_name>`: List all files in a specific bucket.
    3.  `upload <bucket_name> <file_path>`: Upload a local file to a specific bucket.

*   **Concepts Tested:** Command-line argument parsing (`argparse`), interacting with a cloud provider SDK (`boto3`), error handling for cloud API calls, and structuring a simple tool.

*   **Solution:**
    ```python
    import argparse
    import boto3
    from botocore.exceptions import NoCredentialsError, ClientError
    import os

    def create_s3_client():
        """Creates and returns an S3 client, handling credential errors."""
        try:
            s3_client = boto3.client('s3')
            # The following call is to verify credentials early
            s3_client.list_buckets() 
            return s3_client
        except NoCredentialsError:
            print("Error: AWS credentials not found.")
            print("Please configure your credentials (e.g., via 'aws configure').")
            return None
        except ClientError as e:
            # Handle other potential client errors like invalid region
            print(f"Error creating S3 client: {e}")
            return None


    def list_all_buckets(s3_client):
        """Lists all S3 buckets."""
        print("S3 Buckets:")
        response = s3_client.list_buckets()
        for bucket in response['Buckets']:
            print(f"  - {bucket['Name']}")


    def list_files_in_bucket(s3_client, bucket_name):
        """Lists all files within a specified S3 bucket."""
        try:
            print(f"Files in bucket '{bucket_name}':")
            paginator = s3_client.get_paginator('list_objects_v2')
            pages = paginator.paginate(Bucket=bucket_name)
            file_count = 0
            for page in pages:
                if 'Contents' in page:
                    for obj in page['Contents']:
                        print(f"  - {obj['Key']} ({obj['Size']} bytes)")
                        file_count += 1
            if file_count == 0:
                print("  (Bucket is empty or does not exist)")
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchBucket':
                print(f"Error: Bucket '{bucket_name}' does not exist.")
            else:
                print(f"An unexpected error occurred: {e}")


    def upload_file_to_bucket(s3_client, bucket_name, file_path):
        """Uploads a local file to an S3 bucket."""
        if not os.path.exists(file_path):
            print(f"Error: Local file not found at '{file_path}'")
            return

        file_name = os.path.basename(file_path)
        
        try:
            print(f"Uploading '{file_path}' to bucket '{bucket_name}' as '{file_name}'...")
            s3_client.upload_file(file_path, bucket_name, file_name)
            print("Upload successful.")
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchBucket':
                print(f"Error: Bucket '{bucket_name}' does not exist.")
            else:
                print(f"An unexpected error occurred: {e}")
        except Exception as e:
            print(f"An unexpected error occurred: {e}")


    def main():
        parser = argparse.ArgumentParser(description="A simple command-line tool to manage AWS S3 buckets.")
        subparsers = parser.add_subparsers(dest="command", required=True, help="Available commands")

        # 'list' command
        subparsers.add_parser('list', help="List all S3 buckets.")

        # 'list-files' command
        list_files_parser = subparsers.add_parser('list-files', help="List all files in a specific bucket.")
        list_files_parser.add_argument('bucket_name', type=str, help="The name of the S3 bucket.")

        # 'upload' command
        upload_parser = subparsers.add_parser('upload', help="Upload a file to a specific bucket.")
        upload_parser.add_argument('bucket_name', type=str, help="The name of the target S3 bucket.")
        upload_parser.add_argument('file_path', type=str, help="The path to the local file to upload.")

        args = parser.parse_args()
        
        s3_client = create_s3_client()
        if not s3_client:
            return # Exit if client creation failed

        if args.command == 'list':
            list_all_buckets(s3_client)
        elif args.command == 'list-files':
            list_files_in_bucket(s3_client, args.bucket_name)
        elif args.command == 'upload':
            upload_file_to_bucket(s3_client, args.bucket_name, args.file_path)

    if __name__ == "__main__":
        # To run this script:
        # 1. Install boto3: pip install boto3
        # 2. Configure AWS CLI: aws configure
        # 3. Run commands:
        #    python your_script.py list
        #    python your_script.py list-files my-bucket-name
        #    python your_script.py upload my-bucket-name ./local-file.txt
        main()
    ```

---

### Part 7: Deep Dive - Follow-up Questions for Scripting Scenarios

This section provides a list of follow-up questions that an interviewer might ask after you've presented a solution to the scripting problems in Part 6. These questions are designed to test the depth of your understanding of the underlying technologies and your ability to consider edge cases, performance, and alternative designs.

#### Follow-up Questions for Scenario 1 (Find and Archive Old Files)

1.  **Alternative Implementation:** How would you implement this script using the `pathlib` module instead of `os`? What are the advantages of using `pathlib`?
2.  **Performance:** If the directory contained millions of files, `os.listdir()` could consume a lot of memory. How would you change the implementation to be more memory-efficient?
3.  **`subprocess` vs. Native:** You used `subprocess` to call `gzip`. What are the pros and cons of this approach versus using Python's built-in `gzip` library?
4.  **Error Handling:** What happens if `shutil.move()` fails because the destination filesystem is full? How would you make the script more robust against this?
5.  **Concurrency:** How would you modify the script to use a `ThreadPoolExecutor` to compress multiple files concurrently? Would this be effective? Why or why not?
6.  **Date/Time:** The script uses file modification time (`mtime`). What are `atime` and `ctime`, and in what scenarios might they be more appropriate?
7.  **Timezones:** The script uses `datetime.now()`, which is timezone-naive. How could this cause problems, and how would you make the script timezone-aware using the `datetime.timezone` module?
8.  **Permissions:** What potential issues related to file permissions might this script encounter when running, and how would you handle them in the code?
9.  **Atomicity:** The process of compressing and then moving is not atomic. How could you make the operation safer to prevent having a compressed file in the source directory if the move fails?
10. **Configuration:** Hardcoding `days_old=14` is inflexible. How would you modify the script to accept this value from a command-line argument using `argparse`?
11. **Logging:** Instead of `print()`, how would you integrate Python's `logging` module to provide different levels of output (e.g., DEBUG, INFO, ERROR) and log to a file?
12. **Symlinks:** How does your script currently handle symbolic links? Would it archive the link or the file it points to? How would you change this behavior?
13. **Recursion:** The current script only scans the top-level directory. How would you modify it to scan all subdirectories recursively using `os.walk()`?
14. **Testing:** How would you write a unit test for this script using `pytest`? What would you need to mock?
15. **Resource Management:** What happens if the script is killed halfway through compressing a large file? How might you handle cleanup of partial files?
16. **Alternative Compression:** How would you modify the script to support other compression formats like `bzip2` or `zip` based on a user's choice?
17. **Dry Run:** How would you add a `--dry-run` flag that prints the actions the script *would* take without actually modifying any files?
18. **Process Management:** In the `subprocess.run` call, what is the purpose of `check=True`? What would happen if you removed it?
19. **Scalability:** Imagine this script needs to run on a server that is constantly writing new log files. What potential race conditions could occur?
20. **Idempotency:** Is the script idempotent? If you run it twice in a row, what happens? How could you ensure it doesn't try to re-archive already archived files?

#### Follow-up Questions for Scenario 2 (Check Website Health)

1.  **HTTP Methods:** The script uses `requests.get()`. What is the difference between GET, POST, and HEAD requests? When would you use a HEAD request for a health check?
2.  **Status Codes:** The script only checks for `200`. What do status codes in the `3xx` (e.g., 301, 302), `4xx` (e.g., 401, 403), and `5xx` (e.g., 500, 503) ranges signify? How would you modify the script to treat `3xx` redirects as "UP"?
3.  **Concurrency:** The script checks URLs sequentially. This is slow. How would you rewrite it using `concurrent.futures.ThreadPoolExecutor` to check 20 URLs at a time?
4.  **Asyncio:** For checking thousands of URLs, threading has overhead. How would you rewrite the script using `asyncio` and the `aiohttp` library for maximum performance?
5.  **Request Sessions:** What is a `requests.Session` object? What performance benefit does it provide when checking multiple URLs on the same domain?
6.  **Headers:** How would you modify the script to send a custom `User-Agent` header with each request?
7.  **Content Verification:** A `200 OK` status doesn't guarantee the site is working correctly. How would you modify the script to also check that the response body contains a specific string (e.g., "Welcome")?
8.  **Configuration:** How would you modify the script to read not just URLs, but also the expected status code and content string from a JSON or YAML configuration file?
9.  **SSL/TLS:** What happens if a site has an invalid SSL certificate? What exception would `requests` raise, and how would you handle it? How can you disable certificate verification (and why is this dangerous)?
10. **Authentication:** How would you modify the script to check an endpoint that requires Basic Authentication?
11. **Timeouts:** The script has a single `timeout=5`. What is the difference between a connect timeout and a read timeout in the `requests` library?
12. **Retries:** How would you implement a retry mechanism so that if a request fails with a timeout or a `5xx` error, the script tries again up to 3 times with a delay between retries?
13. **Logging vs. Printing:** How would you refactor this to use the `logging` module, writing "UP" statuses to an INFO level and "DOWN" statuses to an ERROR level?
14. **Data Structures:** How would you refactor the function to return a list of dictionaries, with each dictionary containing the URL, status, status code, and response time, instead of just printing the results?
15. **Proxies:** How would you configure `requests` to send its traffic through an HTTP proxy?
16. **Streaming:** If you were checking a URL that returns a very large file, how could you use streaming requests to avoid loading the entire response into memory?
17. **Error Granularity:** The `RequestException` is very broad. What are some more specific exceptions in the `requests` library (like `ConnectionError`, `Timeout`, `HTTPError`) and how would you handle them differently?
18. **Command-Line Interface:** How would you use `argparse` to allow the user to specify the input file path and the timeout value from the command line?
19. **Output Format:** How would you add a feature to output the results as a JSON or CSV file instead of just printing to the console?
20. **Statefulness:** How would you modify the script to only send an alert if a site's status changes (e.g., it was "UP" on the last run but is "DOWN" now)? This would require storing the state between runs.

#### Follow-up Questions for Scenario 3 (Manage AWS S3)

1.  **Authentication:** The script relies on implicit credential discovery. What are the different ways `boto3` can find AWS credentials, and in what order does it search for them?
2.  **Error Handling:** The code catches `ClientError`. What kind of information is available in this exception object that can help you write more granular error handling (e.g., distinguishing "Access Denied" from "Not Found")?
3.  **Paginators:** The `list-files` command uses a paginator. Why is this necessary? What would happen if you used `list_objects_v2` directly on a bucket with 10,000 files?
4.  **Waiters:** If you were creating a new S3 bucket, the operation is not instantaneous. What is a `boto3` "waiter" and how would you use one to block your script until the bucket exists?
5.  **`argparse`:** What is the difference between `add_parser` and `add_subparsers`? Why are subparsers a good fit for this kind of tool?
6.  **Resource vs. Client:** `boto3` offers two levels of API: the client API and the resource API. What is the difference between them, and when might you prefer one over the other?
7.  **Streaming Uploads:** The `upload_file` method is a high-level managed transfer. How would you upload a very large file by streaming it from another source without saving it to disk first, using `upload_fileobj`?
8.  **Pre-signed URLs:** How would you add a new command, `share <bucket_name> <key>`, that generates a temporary, pre-signed URL to grant time-limited read access to a private S3 object?
9.  **Configuration:** How would you allow the user to specify the AWS region and profile from the command line?
10. **Testing:** How would you unit test the `list_all_buckets` function without making real API calls to AWS? (Hint: `botocore.stub.Stubber`).
11. **Performance:** For uploading thousands of small files, creating a new `upload_file` call for each is inefficient. How could you use `concurrent.futures` to parallelize the uploads?
12. **Multipart Uploads:** What is a multipart upload in S3? Does `upload_file` use it automatically? How does it improve reliability and performance for large files?
13. **Idempotency:** If you run the `upload` command twice with the same file, it will re-upload it. How could you modify the script to first check if an object with the same key and content already exists to avoid the re-upload? (Hint: ETag/MD5).
14. **Security:** What is the "Principle of Least Privilege" and how would you apply it when creating an IAM policy for the user/role running this script?
15. **Object Metadata:** How would you modify the `upload` command to include custom metadata (e.g., `x-amz-meta-author: "your-name"`) with the object?
16. **Listing with Prefixes:** How would you change the `list-files` command to function like a directory, allowing the user to list only files within a specific "folder" (prefix) in the bucket?
17. **Deletion:** How would you implement a `delete <bucket_name> <key>` command? What precautions should you take?
18. **Client Configuration:** How can you configure the `boto3` client to set custom timeouts or retry strategies?
19. **Exit Codes:** The script uses `sys.exit(1)` on error. Why is it important for automation scripts to use non-zero exit codes on failure?
20. **Packaging:** How would you package this script using `pyproject.toml` and a tool like `setuptools` so that it could be installed via `pip` and run as a system command?

---

### Part 8: Advanced DevOps Scripting Scenarios & Questions

This section presents more complex, multi-domain scripting challenges that mirror the work of a senior DevOps or SRE. These problems require combining knowledge of APIs, data structures, and cloud services to create robust automation.

#### Scenario 4 (Advanced): Find Failing Kubernetes Pods

*   **Problem:** Write a Python script that uses the `kubernetes` client library to scan a given namespace. The script should find all pods that are in a `CrashLoopBackOff` state and have a restart count greater than a specified threshold (e.g., 5 restarts). For each failing pod found, it should print the pod name, its restart count, and the reason for the last termination.

*   **Concepts Tested:** Interacting with the Kubernetes API via a client library, parsing complex nested object data, conditional logic, and formatted output.

*   **Solution:**
    ```python
    from kubernetes import client, config
    from kubernetes.client.rest import ApiException
    import sys

    def find_crashing_pods(namespace, restart_threshold=5):
        """
        Finds pods in a CrashLoopBackOff state with a high restart count.
        """
        try:
            # Load Kubernetes configuration (from ~/.kube/config or in-cluster)
            config.load_kube_config()
            api = client.CoreV1Api()
        except config.ConfigException:
            print("Error: Could not load Kubernetes configuration.")
            print("Ensure your kubeconfig file is correctly set up.")
            sys.exit(1)

        print(f"--- Scanning namespace '{namespace}' for pods with > {restart_threshold} restarts ---")
        
        try:
            pod_list = api.list_namespaced_pod(namespace, watch=False)
        except ApiException as e:
            if e.status == 404:
                print(f"Error: Namespace '{namespace}' not found.")
            else:
                print(f"Error listing pods in namespace '{namespace}': {e}")
            return

        found_crashing_pod = False
        for pod in pod_list.items:
            # We are interested in pods that are running but have crashing containers
            if pod.status.phase != 'Running' or not pod.status.container_statuses:
                continue

            for container_status in pod.status.container_statuses:
                restart_count = container_status.restart_count
                
                if restart_count > restart_threshold:
                    # Check if the container is in a waiting state with CrashLoopBackOff
                    if container_status.state.waiting and container_status.state.reason == 'CrashLoopBackOff':
                        found_crashing_pod = True
                        print(f"\n[FAIL] Pod: {pod.metadata.name}")
                        print(f"  Container: {container_status.name}")
                        print(f"  Restarts: {restart_count}")
                        
                        # Get the reason for the last termination
                        if container_status.last_state and container_status.last_state.terminated:
                            term_state = container_status.last_state.terminated
                            print(f"  Last Exit Code: {term_state.exit_code}")
                            print(f"  Last Reason: {term_state.reason}")
                            print(f"  Last Message: {term_state.message or 'N/A'}")
        
        if not found_crashing_pod:
            print("No pods found matching the crash criteria.")
        
        print("\n--- Scan Complete ---")

    if __name__ == "__main__":
        # To run this:
        # 1. pip install kubernetes
        # 2. Ensure your kubectl is configured to point to a cluster
        # 3. python your_script.py <namespace>
        if len(sys.argv) != 2:
            print("Usage: python your_script.py <namespace>")
            sys.exit(1)
        
        target_namespace = sys.argv[1]
        find_crashing_pods(target_namespace)
    ```

*   **Follow-up Questions:**
    1.  **Authentication:** The script uses `config.load_kube_config()`. What is the other primary method for authenticating to the Kubernetes API from within a pod, and how would you modify the script to use it?
    2.  **API Efficiency:** For a cluster with thousands of pods, `list_namespaced_pod` can be slow. How could you use `field_selector` or `label_selector` to ask the API server to pre-filter the results?
    3.  **Data Structure:** The `pod` object is deeply nested. What is a good way to explore its structure if you don't have the documentation handy?
    4.  **Alternative Tools:** How could you achieve a similar result using only `kubectl` with its `jsonpath` output format and a shell script? What are the advantages of using the Python client library over this?
    5.  **Watching for Changes:** The script is a one-time snapshot. How would you use the `watch` mechanism in the Kubernetes client to create a long-running script that reports on crashing pods in real-time?
    6.  **Error vs. Waiting:** The script checks for `state.waiting`. What is the difference between a container's `waiting`, `running`, and `terminated` states?
    7.  **Exit Codes:** What is the significance of exit code `137` vs. `1` in a terminated container?
    8.  **Output Format:** How would you modify the script to output the results as a JSON array, suitable for consumption by another tool?
    9.  **Contexts:** How would you modify the script to allow the user to specify which Kubernetes context to use from their kubeconfig file?
    10. **Resource Versions:** What is a `resourceVersion` in the Kubernetes API, and how can it be used for efficient polling?
    11. **Custom Resources (CRDs):** How would you adapt this script to check the status of a Custom Resource instead of a standard Pod?
    12. **Concurrency:** If you needed to scan 100 different namespaces, how would you parallelize the `list_namespaced_pod` calls?
    13. **Logging:** How could you extend the script to automatically fetch the logs from the previously terminated container when it finds a `CrashLoopBackOff`?
    14. **Robustness:** What happens if the `pod.status` or `container_statuses` are `None`? How can you write the accessors more defensively?
    15. **Threshold Configuration:** How would you use `argparse` to make the `restart_threshold` configurable from the command line?
    16. **API Verbs:** The script uses the `list` verb. What are other common Kubernetes API verbs?
    17. **Namespaces:** How would you modify the script to scan *all* namespaces in the cluster?
    18. **Owner References:** How could you extend the script to also report the name and kind of the object that "owns" the failing pod (e.g., the ReplicaSet or Deployment)?
    19. **Packaging:** How would you package this tool so it could be installed with `pip`?
    20. **Actionable Output:** Instead of just printing, how could you integrate this script with an alerting system (e.g., by sending a formatted message to a Slack webhook)?

#### Scenario 5 (Advanced): Automate Cloud Resource Cleanup

*   **Problem:** Write a Python script using `boto3` that scans all running EC2 instances in a specific AWS region. The script should identify any instance that is missing a required tag (e.g., a tag with the key `owner`). For each non-compliant instance, it should add a new tag, `cleanup-candidate`, with the value set to today's date.

*   **Concepts Tested:** Cloud SDK usage (`boto3`), resource filtering, resource tagging, and practical automation logic.

*   **Solution:**
    ```python
    import boto3
    from botocore.exceptions import ClientError
    import argparse
    from datetime import date

    def tag_untagged_instances(region, required_tag_key='owner', dry_run=False):
        """
        Finds running EC2 instances missing a specific tag and tags them for cleanup.
        """
        try:
            ec2 = boto3.client('ec2', region_name=region)
        except ClientError as e:
            print(f"Error creating EC2 client in region '{region}': {e}")
            return

        print(f"--- Scanning for running instances in region '{region}' missing tag '{required_tag_key}' ---")
        
        # Filter for instances that are in the 'running' state
        filters = [{'Name': 'instance-state-name', 'Values': ['running']}]
        
        try:
            paginator = ec2.get_paginator('describe_instances')
            pages = paginator.paginate(Filters=filters)
            
            instances_to_tag = []
            
            for page in pages:
                for reservation in page['Reservations']:
                    for instance in reservation['Instances']:
                        instance_id = instance['InstanceId']
                        tags = {tag['Key']: tag['Value'] for tag in instance.get('Tags', [])}
                        
                        if required_tag_key not in tags:
                            print(f"[NON-COMPLIANT] Instance ID: {instance_id} is missing the '{required_tag_key}' tag.")
                            instances_to_tag.append(instance_id)

            if not instances_to_tag:
                print("All running instances are compliant.")
                return

            print(f"\nFound {len(instances_to_tag)} non-compliant instances.")
            
            if dry_run:
                print("DRY RUN: Would tag the above instances with 'cleanup-candidate'. No action taken.")
            else:
                print("Tagging instances with 'cleanup-candidate'...")
                cleanup_tag = {'Key': 'cleanup-candidate', 'Value': str(date.today())}
                ec2.create_tags(Resources=instances_to_tag, Tags=[cleanup_tag])
                print("Tagging complete.")

        except ClientError as e:
            print(f"An AWS API error occurred: {e}")
        except Exception as e:
            print(f"An unexpected error occurred: {e}")

    if __name__ == "__main__":
        parser = argparse.ArgumentParser(description="Tag untagged EC2 instances for cleanup.")
        parser.add_argument('--region', type=str, required=True, help="The AWS region to scan.")
        parser.add_argument('--tag', type=str, default='owner', help="The required tag key to check for.")
        parser.add_argument('--dry-run', action='store_true', help="Run the script without making any changes.")
        
        args = parser.parse_args()
        
        tag_untagged_instances(args.region, args.tag, args.dry_run)
    ```

*   **Follow-up Questions:**
    1.  **Filtering:** The script filters instances on the client side. How could you use the `Filters` parameter in `describe_instances` to ask the AWS API to perform the filtering for you? Is it possible to filter for the *absence* of a tag key on the server side?
    2.  **Performance:** If you have thousands of instances, `describe_instances` can be slow. What is the `MaxResults` parameter, and how does it relate to pagination?
    3.  **Rate Limiting:** What happens if you run this script against an account with tens of thousands of instances and make too many API calls too quickly? What is "throttling" and how can you handle it with `boto3`?
    4.  **Cost:** What other resources, besides EC2 instances, are major contributors to cloud costs and could be targeted by similar cleanup scripts? (e.g., EBS volumes, S3 buckets, ELB).
    5.  **Alternative Action:** Instead of tagging, how would you modify the script to stop the instances? Why might tagging be a safer first step?
    6.  **IAM Policy:** Write an IAM policy that grants the minimum permissions required for this script to run.
    7.  **Multi-Account/Multi-Region:** How would you modify this script to run across multiple AWS accounts and all available EC2 regions?
    8.  **Lambda:** This script is run manually. How would you package it to run automatically as a daily AWS Lambda function triggered by EventBridge?
    9.  **State:** The script is stateless. How could you use a DynamoDB table to keep track of instances that have been tagged, and send a notification if an instance remains a cleanup candidate for more than 7 days?
    10. **Resource APIs:** You've used the `boto3` client. How would the code look different if you used the higher-level Resource API?
    11. **Tagging Atomicity:** The `create_tags` call can tag up to 1000 resources at once. What happens if the call fails halfway through? Is the operation atomic?
    12. **Data Classes:** The `instance` object is a raw dictionary. How could you use Python's `dataclasses` to create a more structured `Instance` object for easier use within your code?
    13. **Testing:** How would you test this script without running it against a real AWS account?
    14. **Exclusions:** How would you add a feature to exclude certain instances from being tagged, even if they are non-compliant (e.g., based on a list of instance IDs in a config file)?
    15. **Human-in-the-Loop:** How could you modify the script to generate a report (e.g., a CSV file) and wait for human approval before applying the tags?
    16. **Tag Value:** The script uses today's date as the tag value. What other information might be useful to include in the tag?
    17. **Cost Allocation Tags:** What are AWS Cost Allocation Tags, and why is the `owner` tag a common example?
    18. **Boto3 Sessions:** What is the difference between creating a default client (`boto3.client`) and creating a client from a `boto3.Session` object?
    19. **Error Reporting:** How could you integrate this with an alerting system to report how many non-compliant instances were found on each run?
    20. **Beyond EC2:** How would you adapt the core logic of this script to find unattached EBS volumes?

#### Scenario 6 (Expert): Reconcile Discrepancies Between Two Systems

*   **Problem:** You are given two data sources: 1) A `users.json` file from an HR system that contains the canonical list of active employees and their GitHub usernames. 2) The GitHub API, from which you can get a list of all members in your GitHub organization. Write a script that reconciles these two sources to find:
    *   Users in the GitHub organization who are **not** in the HR file (potential offboarding candidates).
    *   Users in the HR file who are **not** in the GitHub organization (potential onboarding candidates).

*   **Concepts Tested:** Data reconciliation, efficient use of data structures (sets), consuming REST APIs, and processing JSON data.

*   **Solution:**
    ```python
    import json
    import requests
    import argparse
    import os

    def get_github_org_members(org_name, github_token):
        """Paginates through the GitHub API to get all members of an organization."""
        members = set()
        page = 1
        per_page = 100
        headers = {'Authorization': f'token {github_token}'}
        
        while True:
            url = f"https://api.github.com/orgs/{org_name}/members?per_page={per_page}&page={page}"
            try:
                response = requests.get(url, headers=headers)
                response.raise_for_status()  # Raise an exception for bad status codes
                data = response.json()
                
                if not data:
                    break # No more members
                
                for member in data:
                    members.add(member['login'].lower())
                
                page += 1
            except requests.RequestException as e:
                print(f"Error fetching GitHub members: {e}")
                return None
        return members

    def get_hr_system_users(filepath):
        """Loads user data from the HR JSON file."""
        try:
            with open(filepath, 'r') as f:
                hr_data = json.load(f)
            
            # Assuming the file is a list of objects, each with a 'github_username' field
            hr_users = {user['github_username'].lower() for user in hr_data if 'github_username' in user}
            return hr_users
        except FileNotFoundError:
            print(f"Error: HR file not found at '{filepath}'")
            return None
        except json.JSONDecodeError:
            print(f"Error: Could not decode JSON from '{filepath}'")
            return None

    def reconcile_users(hr_users, github_users):
        """Compares the two sets of users and prints discrepancies."""
        if hr_users is None or github_users is None:
            print("Cannot perform reconciliation due to previous errors.")
            return

        print("\n--- User Reconciliation Report ---")
        
        # Users in GitHub but not in HR system (Offboarding candidates)
        offboarding_needed = github_users - hr_users
        if offboarding_needed:
            print(f"\n[!] {len(offboarding_needed)} users in GitHub but not in HR system (Offboarding candidates):")
            for user in sorted(list(offboarding_needed)):
                print(f"  - {user}")
        else:
            print("\n[OK] All GitHub users are present in the HR system.")

        # Users in HR system but not in GitHub (Onboarding candidates)
        onboarding_needed = hr_users - github_users
        if onboarding_needed:
            print(f"\n[!] {len(onboarding_needed)} users in HR system but not in GitHub (Onboarding candidates):")
            for user in sorted(list(onboarding_needed)):
                print(f"  - {user}")
        else:
            print("\n[OK] All HR users are present in the GitHub organization.")
            
        print("\n--- Report Complete ---")

    if __name__ == "__main__":
        # Create a dummy HR file for testing
        dummy_hr_data = [
            {"employee_id": 101, "name": "Alice", "github_username": "alice_gh"},
            {"employee_id": 102, "name": "Bob", "github_username": "bob_codes"},
            {"employee_id": 103, "name": "Charlie", "github_username": "charlie_dev"} # Onboarding candidate
        ]
        with open("users.json", "w") as f:
            json.dump(dummy_hr_data, f)

        parser = argparse.ArgumentParser(description="Reconcile users between HR system and GitHub.")
        parser.add_argument('--org', required=True, help="GitHub organization name.")
        parser.add_argument('--hr-file', default='users.json', help="Path to the HR system's JSON user file.")
        
        args = parser.parse_args()
        
        github_token = os.environ.get('GITHUB_TOKEN')
        if not github_token:
            print("Error: GITHUB_TOKEN environment variable not set.")
        else:
            hr_users_set = get_hr_system_users(args.hr_file)
            # In a real scenario, you'd call get_github_org_members.
            # For a test, we can mock the return value.
            # github_users_set = get_github_org_members(args.org, github_token)
            
            # Mocked GitHub data for a runnable example:
            github_users_set = {"alice_gh", "bob_codes", "eve_hacker"} # eve_hacker is an offboarding candidate
            
            reconcile_users(hr_users_set, github_users_set)
    ```

*   **Follow-up Questions:**
    1.  **Data Structures:** Why is a `set` the ideal data structure for this problem? What would be the performance implication of using lists instead?
    2.  **API Rate Limiting:** The GitHub API has a rate limit. How would you modify the script to respect the `X-RateLimit-Remaining` and `X-RateLimit-Reset` headers in the API response?
    3.  **Authentication:** The script uses a Personal Access Token. What are the security pros and cons of this method versus using a GitHub App?
    4.  **Case Insensitivity:** The script converts usernames to lowercase. Why is this important for this kind of reconciliation?
    5.  **Data Source Reliability:** What happens if the GitHub API is down or the HR file is malformed? How have you handled this?
    6.  **Alternative Data Formats:** How would you change the script if the HR data was provided as a CSV file instead of JSON?
    7.  **Pagination:** Explain why pagination is necessary when calling the GitHub API and how the `while True` loop implements it.
    8.  **Set Operations:** The script uses the set difference operator (`-`). What do the `&` (intersection), `|` (union), and `^` (symmetric difference) operators do, and what information would they provide in this scenario?
    9.  **Actionability:** The script only prints a report. How would you extend it to take action, for example, by opening a Jira ticket for each onboarding/offboarding candidate?
    10. **Caching:** Calling the GitHub API can be slow. How could you implement a local cache (e.g., using a simple file or `shelve`) to store the GitHub member list for a few hours to avoid repeated API calls?
    11. **Error Handling:** The `raise_for_status()` call is a good practice. What kind of exceptions does it raise?
    12. **Configuration:** How would you refactor the script to pull configuration like the API endpoint and organization name from a separate `config.yaml` file?
    13. **Testing:** How would you unit test the `reconcile_users` function? How would you test `get_github_org_members` without making real API calls (Hint: `requests-mock` library)?
    14. **HR Data Schema:** The script assumes a specific key `github_username`. How would you make the script more robust if this key name could change or be absent for some records?
    15. **Logging:** How would you use the `logging` module to log the progress of the script, including how many pages were fetched from the GitHub API?
    16. **Exceptions/Allow-listing:** How would you implement an "allow-list" to ignore certain accounts during reconciliation (e.g., shared bot accounts that don't exist in HR)?
    17. **Directionality:** The script performs two separate comparisons. Could you get all the information you need with a single set operation?
    18. **Scalability:** What are the performance bottlenecks in this script if the organization has 50,000 members?
    19. **Secrets Management:** The script reads the GitHub token from an environment variable. Why is this better than hardcoding it? What are even more secure ways to manage this secret?
    20. **Delta Reports:** How would you modify the script to only report *new* discrepancies since the last run?
