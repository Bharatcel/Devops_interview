# Chapter 18: Python Masterclass - From Fundamentals to Advanced Engineering

## Abstract

Python's simplicity, readability, and vast ecosystem have made it one of the most dominant programming languages in the world, powering everything from web servers and data science models to infrastructure automation and machine learning. For a DevOps/SRE engineer, a deep understanding of Python is not just a "nice-to-have"; it is a fundamental requirement for writing automation scripts, building internal tools, and interacting with cloud provider SDKs. This chapter provides a definitive, book-level guide to Python, designed for engineers. We will move from the foundational syntax to advanced concepts like concurrency, decorators, and the data model, and conclude with practical interview scenarios that test your ability to solve real-world problems with idiomatic Python.

---

### Part 1: Python Fundamentals - The Bedrock

#### 1.1 The Zen of Python & Environment Setup

*   **The Zen of Python (`import this`):** Python's design philosophy is captured in 19 guiding principles, including "Beautiful is better than ugly," "Simple is better than complex," and "Readability counts." This emphasis on clarity is a core feature of the language.
*   **Interpreter:** Python is an interpreted language. The CPython interpreter is the standard implementation.
*   **Virtual Environments (`venv`):** You should **never** use the system's global Python for project development. A virtual environment is an isolated Python environment that allows you to manage dependencies for a specific project independently.
    *   **Creation:** `python -m venv venv`
    *   **Activation:** `source venv/bin/activate` (Linux/macOS) or `.\venv\Scripts\activate` (Windows)
*   **Package Management (`pip`):** `pip` is Python's package installer. You use it to install libraries from the Python Package Index (PyPI).
    *   **Installation:** `pip install requests`
    *   **Dependency Freezing:** `pip freeze > requirements.txt`
    *   **Installing from file:** `pip install -r requirements.txt`

#### 1.2 Core Data Types and Structures

*   **Primitive Types:**
    *   **Numbers:** `int`, `float`.
    *   **Strings:** Immutable sequences of characters. `f-strings` (e.g., `f"Hello, {name}"`) are the modern, preferred way to format strings.
    *   **Booleans:** `True`, `False`.
*   **Data Structures (The "Collections"):**
    *   **List:** A mutable, ordered sequence of elements. `my_list = [1, "a", 3.0]`
        *   *Use Case:* When you need an ordered collection that you can modify.
    *   **Tuple:** An immutable, ordered sequence of elements. `my_tuple = (1, "a", 3.0)`
        *   *Use Case:* When you need an ordered collection that should not be changed (e.g., returning multiple values from a function, dictionary keys).
    *   **Dictionary (`dict`):** A mutable collection of key-value pairs. Keys must be immutable types (like strings or tuples). `my_dict = {"name": "Alice", "age": 30}`
        *   *Use Case:* Storing and retrieving data by a unique key. The fundamental mapping type in Python.
    *   **Set:** A mutable, unordered collection of *unique* elements. `my_set = {1, 2, 3}`
        *   *Use Case:* Checking for membership (`in`) or removing duplicates from a list.

#### 1.3 Control Flow & Functions

*   **Conditional Logic:** `if`, `elif`, `else`.
*   **Loops:**
    *   `for` loop: Iterates over a sequence (e.g., a list, a string, or a `range`).
    *   `while` loop: Executes as long as a condition is true.
*   **Functions:**
    *   Defined with the `def` keyword.
    *   **Arguments:** Can have positional arguments, keyword arguments (with default values).
    *   **`*args` and `**kwargs`:** A powerful feature for writing functions that accept a variable number of arguments.
        *   `*args`: Collects extra positional arguments into a tuple.
        *   `**kwargs`: Collects extra keyword arguments into a dictionary.
        ```python
        def my_function(a, b, *args, **kwargs):
            print(f"a={a}, b={b}")
            print(f"args={args}") # e.g., (3, 4)
            print(f"kwargs={kwargs}") # e.g., {'c': 5, 'd': 6}

        my_function(1, 2, 3, 4, c=5, d=6)
        ```
    *   **Scope:** Python has LEGB scope resolution: **L**ocal, **E**nclosing, **G**lobal, **B**uilt-in.

#### 1.4 Error Handling

*   The `try...except` block is used to handle exceptions gracefully.
    ```python
    try:
        result = 10 / 0
    except ZeroDivisionError as e:
        print(f"Error: Cannot divide by zero. Details: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
    finally:
        print("This block always executes, for cleanup.")
    ```

---

### Part 2: Intermediate Python & Software Engineering

#### 2.1 Object-Oriented Programming (OOP)

*   **Classes and Objects:** A class is a blueprint for creating objects. An object is an instance of a class.
*   **The `__init__` method:** The constructor for a class. It's called when a new object is created. `self` refers to the instance of the object being created.
*   **The Four Pillars of OOP:**
    1.  **Encapsulation:** Bundling data (attributes) and methods that operate on the data into a single unit (a class). Using "private" attributes (by convention, prefixed with an underscore, e.g., `_internal_var`) hides implementation details.
    2.  **Inheritance:** Creating a new class (child class) from an existing class (parent class). The child class inherits the attributes and methods of the parent.
    3.  **Polymorphism:** The ability of different classes to be treated as instances of a common superclass. For example, if `Dog` and `Cat` both inherit from `Animal` and both have a `speak()` method, you can call `animal.speak()` without knowing if it's a dog or a cat.
    4.  **Abstraction:** Hiding complex implementation details and exposing only the necessary features of an object.

#### 2.2 Modules and Packages

*   **Module:** Any `.py` file is a module.
*   **Package:** A directory containing modules. It **must** contain a file named `__init__.py` (which can be empty) to be considered a package. This allows you to structure your code logically.
    ```
    my_project/
    ├── main.py
    └── my_package/
        ├── __init__.py
        ├── module1.py
        └── module2.py
    ```
    In `main.py`, you can then do `from my_package import module1`.

#### 2.3 Comprehensions & Generators

*   **Comprehensions:** A concise, readable way to create lists, dicts, or sets.
    *   **List Comprehension:** `squares = [x**2 for x in range(10)]`
    *   **Dict Comprehension:** `square_dict = {x: x**2 for x in range(10)}`
*   **Generators:** Generators look like functions but use the `yield` keyword to return data. They produce items one at a time and only when requested. This makes them incredibly memory-efficient for working with large data sets.
    *   **Generator Function:**
        ```python
        def count_up_to(max):
            count = 1
            while count <= max:
                yield count
                count += 1
        ```
    *   **Generator Expression:** Similar to a list comprehension, but with parentheses. It creates a generator object without building the full list in memory.
        `lazy_squares = (x**2 for x in range(1000000))`

#### 2.4 Decorators

*   A decorator is a function that takes another function as an argument, adds some functionality, and returns another function, all without altering the source code of the original function.
*   *Use Case:* Logging, timing, authentication checks.
    ```python
    import time

    def timing_decorator(func):
        def wrapper(*args, **kwargs):
            start_time = time.time()
            result = func(*args, **kwargs)
            end_time = time.time()
            print(f"Function {func.__name__} took {end_time - start_time:.4f} seconds")
            return result
        return wrapper

    @timing_decorator
    def slow_function():
        time.sleep(2)

    slow_function() # Will print the execution time
    ```

---

### Part 3: Advanced Python

#### 3.1 Concurrency Models

*   **Threading:** Uses threads to run tasks concurrently. In CPython, due to the **Global Interpreter Lock (GIL)**, only one thread can execute Python bytecode at a time. This means threading is great for **I/O-bound** tasks (like making network requests), where threads can wait for I/O without blocking each other, but it does not provide true parallelism for **CPU-bound** tasks.
*   **Multiprocessing:** Bypasses the GIL by creating separate processes, each with its own Python interpreter and memory space. This allows for true parallel execution on multi-core CPUs. It's ideal for **CPU-bound** tasks (like data processing or calculations).
*   **Asyncio:** A single-threaded, single-process approach to concurrency that uses an **event loop**. It's designed for a large number of I/O-bound tasks. Using `async` and `await` syntax, you can write code that "pauses" when it's waiting for I/O and allows the event loop to run other tasks. It can be much more efficient than threading for a very high number of concurrent connections.

#### 3.2 Context Managers (`with` statement)

*   The `with` statement simplifies resource management by ensuring that cleanup logic (like closing a file or a network connection) is always executed. Any object that has `__enter__` and `__exit__` methods can be used as a context manager.
    ```python
    # The right way to open a file
    with open("my_file.txt", "w") as f:
        f.write("Hello, world!")
    # The file is automatically closed here, even if errors occur inside the block.
    ```

#### 3.3 The Python Data Model (Dunder Methods)

*   "Dunder" (double underscore) methods like `__init__`, `__str__`, `__len__`, and `__add__` allow your custom objects to integrate with Python's built-in syntax and functions. Implementing `__len__` lets you call `len(my_object)`. Implementing `__add__` lets you use the `+` operator on your objects. This is the key to making your classes feel "Pythonic."

---

### Part 4: The Python Ecosystem

*   **Web Development:**
    *   **Flask:** A lightweight "micro-framework" that is excellent for building APIs and smaller web applications.
    *   **Django:** A powerful, "batteries-included" framework that provides an ORM, admin interface, and everything you need for large, complex web applications.
*   **Data Science & Machine Learning:**
    *   **NumPy:** The fundamental package for numerical computing, providing a powerful N-dimensional array object.
    *   **Pandas:** Provides high-performance, easy-to-use data structures (like the DataFrame) and data analysis tools.
    *   **Scikit-learn:** A comprehensive library of machine learning algorithms.
*   **Testing:**
    *   **pytest:** The de facto standard for testing in Python. It has a simple, clean syntax for writing tests and a powerful plugin ecosystem.

---

### Part 5: Practice Questions & Scenarios

#### Q1 (Basic): What will be the output of `print(my_list)`?
```python
my_list = [1, 2, 3]
my_other_list = my_list
my_other_list.append(4)
print(my_list)
```
**Answer:** The output will be `[1, 2, 3, 4]`. This is because lists are mutable objects. `my_other_list = my_list` does not create a copy of the list; it makes both variables point to the *same* list object in memory. To create a shallow copy, you would use `my_other_list = my_list.copy()` or `my_other_list = my_list[:]`.

#### Q2 (Intermediate): Write a list comprehension to create a list of all even numbers from 0 to 20.
**Answer:**
```python
evens = [x for x in range(21) if x % 2 == 0]
```

#### Q3 (Intermediate): What is the GIL and how does it affect Python performance?
**Answer:** The GIL, or Global Interpreter Lock, is a mutex (a lock) in the CPython interpreter that protects access to Python objects, preventing multiple threads from executing Python bytecode at the same time. The consequence is that even on a multi-core processor, a multi-threaded Python program can only use one core at a time for executing Python code. This makes threading unsuitable for speeding up CPU-bound tasks. However, it is still very effective for I/O-bound tasks, because the GIL is released when a thread is waiting for I/O (like a network response), allowing another thread to run. To achieve true CPU parallelism, you must use the `multiprocessing` module.

#### Q4 (Advanced/Scenario): You are writing a script to process a 10 GB log file and count the number of lines containing the word "ERROR". Your first attempt is this:
```python
def count_errors(filename):
    with open(filename, 'r') as f:
        lines = f.readlines()
    error_count = 0
    for line in lines:
        if "ERROR" in line:
            error_count += 1
    return error_count
```
**Why is this a bad approach for a large file, and how would you rewrite it to be memory-efficient?**
**Answer:** This is a bad approach because `f.readlines()` reads the *entire* 10 GB file into memory at once, which will likely crash the machine or cause it to swap heavily.

The memory-efficient, "Pythonic" way to solve this is to iterate over the file object directly. The file object acts as a generator, reading the file line-by-line without loading it all into memory.

**First Improvement (Line-by-line iteration):**
```python
def count_errors_v2(filename):
    error_count = 0
    with open(filename, 'r') as f:
        for line in f: # This iterates line-by-line
            if "ERROR" in line:
                error_count += 1
    return error_count
```

**Second Improvement (Using a generator expression and `sum`):**
This is an even more concise and idiomatic solution. It uses a generator expression to create a sequence of `1`s for each error line, and then `sum` adds them up. This is still fully memory-efficient.
```python
def count_errors_v3(filename):
    with open(filename, 'r') as f:
        return sum(1 for line in f if "ERROR" in line)
```

#### Q5 (Advanced/Scenario): You need to write a function that fetches data from a list of 100 different URLs. The naive approach is to loop through the URLs and fetch them one by one, but this is very slow. How would you use Python's concurrency features to speed this up significantly? Provide code examples for two different approaches.
**Answer:** This is a classic I/O-bound problem, perfect for concurrency. A sequential approach is slow because the program spends most of its time waiting for network responses.

**Approach 1: Using `concurrent.futures` with Threading (Good for simplicity)**
The `concurrent.futures` module provides a high-level interface for asynchronously executing callables. `ThreadPoolExecutor` is ideal for this I/O-bound task.

```python
import concurrent.futures
import requests

URLS = ["http://example.com"] * 100 # A list of 100 URLs

def fetch_url(url):
    try:
        response = requests.get(url, timeout=10)
        return response.status_code
    except requests.RequestException as e:
        return str(e)

# Use a with statement to ensure threads are cleaned up
with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
    # map() runs the function for each item in the iterable
    results = executor.map(fetch_url, URLS)

for url, result in zip(URLS, results):
    print(f"URL: {url}, Result: {result}")
```
This is much faster because up to 20 requests are happening concurrently. While one thread is waiting for a response from a server, another thread can be sending a request to a different server.

**Approach 2: Using `asyncio` and `aiohttp` (Best for very high concurrency)**
For a very large number of URLs (thousands or more), `asyncio` can be even more efficient than threading as it has less overhead per task. It requires using an async-compatible HTTP library like `aiohttp`.

```python
import asyncio
import aiohttp

URLS = ["http://example.com"] * 100

async def fetch_url_async(session, url):
    try:
        async with session.get(url, timeout=10) as response:
            return await response.read() # Just get the content
    except Exception as e:
        return str(e)

async def main():
    # Create a single session for all requests
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url_async(session, url) for url in URLS]
        # asyncio.gather runs all tasks concurrently
        results = await asyncio.gather(*tasks)
        for url, result in zip(URLS, results):
            # Printing just the length to avoid huge output
            print(f"URL: {url}, Result length: {len(result)}")

# Run the main async function
asyncio.run(main())
```
This approach uses a single thread and an event loop to manage all 100 requests concurrently. It's often the highest-performance solution for I/O-bound problems in Python.

---

### Part 6: Python for DevOps - Scripting Interview Questions

This section focuses on practical, hands-on scripting problems that are representative of what a DevOps or SRE engineer does day-to-day. These questions test your ability to use Python's standard library and common third-party packages to automate real-world tasks.

#### Scenario 1 (Basic): Find and Archive Old Files

*   **Problem:** Write a script that scans a given directory for files ending with `.log`. Any log file older than 14 days should be compressed with `gzip` and moved to an `archive` subdirectory. The script should create the `archive` directory if it doesn't exist.

*   **Concepts Tested:** File system navigation (`os`, `pathlib`), date and time calculations (`datetime`), file manipulation (`shutil`), and running external processes (`subprocess`).

*   **Solution:**
    ```python
    import os
    import sys
    import shutil
    import subprocess
    from datetime import datetime, timedelta

    def archive_old_logs(directory_path, days_old=14):
        """
        Scans a directory for .log files, compresses and archives those older
        than a specified number of days.
        """
        # Ensure the target directory is valid
        if not os.path.isdir(directory_path):
            print(f"Error: Directory not found at '{directory_path}'")
            sys.exit(1)

        # Create the archive directory if it doesn't exist
        archive_dir = os.path.join(directory_path, "archive")
        os.makedirs(archive_dir, exist_ok=True)

        # Calculate the cutoff time
        cutoff_date = datetime.now() - timedelta(days=days_old)

        print(f"Scanning '{directory_path}' for log files older than {cutoff_date}...")

        # Walk through the directory
        for filename in os.listdir(directory_path):
            if filename.endswith(".log"):
                file_path = os.path.join(directory_path, filename)
                
                # Get file modification time
                file_mod_time = datetime.fromtimestamp(os.path.getmtime(file_path))

                if file_mod_time < cutoff_date:
                    print(f"  -> Archiving '{filename}'...")
                    
                    # Compress the file using gzip
                    try:
                        subprocess.run(["gzip", file_path], check=True)
                        
                        # Move the compressed file to the archive directory
                        gzipped_filename = f"{filename}.gz"
                        gzipped_filepath = os.path.join(directory_path, gzipped_filename)
                        shutil.move(gzipped_filepath, archive_dir)
                        
                        print(f"     '{gzipped_filename}' moved to '{archive_dir}'")
                    
                    except subprocess.CalledProcessError as e:
                        print(f"     Error compressing '{filename}': {e}")
                    except FileNotFoundError:
                        print(f"     Error: 'gzip' command not found. Is it installed?")
                    except Exception as e:
                        print(f"     An unexpected error occurred: {e}")

    if __name__ == "__main__":
        # Example usage: python your_script.py /path/to/logs
        if len(sys.argv) != 2:
            print("Usage: python your_script.py <directory_path>")
            sys.exit(1)
        
        target_directory = sys.argv[1]
        archive_old_logs(target_directory)
    ```

#### Scenario 2 (Intermediate): Check Website Health and Report Status

*   **Problem:** Write a script that reads a list of URLs from a `urls.txt` file. For each URL, it should make an HTTP GET request. If the status code is `200 OK`, it should log that the site is "UP". Otherwise, it should log that the site is "DOWN" along with the status code. The script should handle network errors gracefully.

*   **Concepts Tested:** File I/O, making HTTP requests (`requests` library), error handling, and basic data processing.

*   **Solution:**
    ```python
    import requests
    import sys

    def check_website_health(filepath):
        """
        Reads URLs from a file and checks their HTTP status.
        """
        try:
            with open(filepath, 'r') as f:
                urls = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"Error: The file '{filepath}' was not found.")
            sys.exit(1)

        if not urls:
            print("The file is empty. No URLs to check.")
            return

        print("--- Website Health Check ---")
        for url in urls:
            try:
                # Make a request with a timeout to prevent hanging
                response = requests.get(url, timeout=5)
                
                if response.status_code == 200:
                    print(f"[ UP ] {url}")
                else:
                    print(f"[DOWN] {url} (Status Code: {response.status_code})")

            except requests.exceptions.Timeout:
                print(f"[DOWN] {url} (Error: Request timed out)")
            except requests.exceptions.ConnectionError:
                print(f"[DOWN] {url} (Error: Connection failed)")
            except requests.exceptions.RequestException as e:
                print(f"[DOWN] {url} (Error: {e})")
        print("--- Check Complete ---")


    if __name__ == "__main__":
        # Create a dummy urls.txt for testing
        with open("urls.txt", "w") as f:
            f.write("https://www.google.com\n")
            f.write("https://www.github.com\n")
            f.write("http://httpbin.org/status/404\n")
            f.write("https://invalid-domain-that-does-not-exist.com\n")

        check_website_health("urls.txt")
    ```

#### Scenario 3 (Advanced): Manage AWS S3 Buckets

*   **Problem:** Write a command-line tool using Python's `argparse` and `boto3` (the AWS SDK for Python) to perform basic S3 operations. The tool should support three actions:
    1.  `list`: List all S3 buckets.
    2.  `list-files <bucket_name>`: List all files in a specific bucket.
    3.  `upload <bucket_name> <file_path>`: Upload a local file to a specific bucket.

*   **Concepts Tested:** Command-line argument parsing (`argparse`), interacting with a cloud provider SDK (`boto3`), error handling for cloud API calls, and structuring a simple tool.

*   **Solution:**
    ```python
    import argparse
    import boto3
    from botocore.exceptions import NoCredentialsError, ClientError
    import os

    def create_s3_client():
        """Creates and returns an S3 client, handling credential errors."""
        try:
            s3_client = boto3.client('s3')
            # The following call is to verify credentials early
            s3_client.list_buckets() 
            return s3_client
        except NoCredentialsError:
            print("Error: AWS credentials not found.")
            print("Please configure your credentials (e.g., via 'aws configure').")
            return None
        except ClientError as e:
            # Handle other potential client errors like invalid region
            print(f"Error creating S3 client: {e}")
            return None


    def list_all_buckets(s3_client):
        """Lists all S3 buckets."""
        print("S3 Buckets:")
        response = s3_client.list_buckets()
        for bucket in response['Buckets']:
            print(f"  - {bucket['Name']}")


    def list_files_in_bucket(s3_client, bucket_name):
        """Lists all files within a specified S3 bucket."""
        try:
            print(f"Files in bucket '{bucket_name}':")
            paginator = s3_client.get_paginator('list_objects_v2')
            pages = paginator.paginate(Bucket=bucket_name)
            file_count = 0
            for page in pages:
                if 'Contents' in page:
                    for obj in page['Contents']:
                        print(f"  - {obj['Key']} ({obj['Size']} bytes)")
                        file_count += 1
            if file_count == 0:
                print("  (Bucket is empty or does not exist)")
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchBucket':
                print(f"Error: Bucket '{bucket_name}' does not exist.")
            else:
                print(f"An unexpected error occurred: {e}")


    def upload_file_to_bucket(s3_client, bucket_name, file_path):
        """Uploads a local file to an S3 bucket."""
        if not os.path.exists(file_path):
            print(f"Error: Local file not found at '{file_path}'")
            return

        file_name = os.path.basename(file_path)
        
        try:
            print(f"Uploading '{file_path}' to bucket '{bucket_name}' as '{file_name}'...")
            s3_client.upload_file(file_path, bucket_name, file_name)
            print("Upload successful.")
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchBucket':
                print(f"Error: Bucket '{bucket_name}' does not exist.")
            else:
                print(f"An unexpected error occurred: {e}")
        except Exception as e:
            print(f"An unexpected error occurred: {e}")


    def main():
        parser = argparse.ArgumentParser(description="A simple command-line tool to manage AWS S3 buckets.")
        subparsers = parser.add_subparsers(dest="command", required=True, help="Available commands")

        # 'list' command
        subparsers.add_parser('list', help="List all S3 buckets.")

        # 'list-files' command
        list_files_parser = subparsers.add_parser('list-files', help="List all files in a specific bucket.")
        list_files_parser.add_argument('bucket_name', type=str, help="The name of the S3 bucket.")

        # 'upload' command
        upload_parser = subparsers.add_parser('upload', help="Upload a file to a specific bucket.")
        upload_parser.add_argument('bucket_name', type=str, help="The name of the target S3 bucket.")
        upload_parser.add_argument('file_path', type=str, help="The path to the local file to upload.")

        args = parser.parse_args()
        
        s3_client = create_s3_client()
        if not s3_client:
            return # Exit if client creation failed

        if args.command == 'list':
            list_all_buckets(s3_client)
        elif args.command == 'list-files':
            list_files_in_bucket(s3_client, args.bucket_name)
        elif args.command == 'upload':
            upload_file_to_bucket(s3_client, args.bucket_name, args.file_path)

    if __name__ == "__main__":
        # To run this script:
        # 1. Install boto3: pip install boto3
        # 2. Configure AWS CLI: aws configure
        # 3. Run commands:
        #    python your_script.py list
        #    python your_script.py list-files my-bucket-name
        #    python your_script.py upload my-bucket-name ./local-file.txt
        main()
    ```

---

### Part 7: Deep Dive - Follow-up Questions for Scripting Scenarios

This section provides a list of follow-up questions that an interviewer might ask after you've presented a solution to the scripting problems in Part 6. These questions are designed to test the depth of your understanding of the underlying technologies and your ability to consider edge cases, performance, and alternative designs.

#### Follow-up Questions for Scenario 1 (Find and Archive Old Files)

1.  **Alternative Implementation:** How would you implement this script using the `pathlib` module instead of `os`? What are the advantages of using `pathlib`?
2.  **Performance:** If the directory contained millions of files, `os.listdir()` could consume a lot of memory. How would you change the implementation to be more memory-efficient?
3.  **`subprocess` vs. Native:** You used `subprocess` to call `gzip`. What are the pros and cons of this approach versus using Python's built-in `gzip` library?
4.  **Error Handling:** What happens if `shutil.move()` fails because the destination filesystem is full? How would you make the script more robust against this?
5.  **Concurrency:** How would you modify the script to use a `ThreadPoolExecutor` to compress multiple files concurrently? Would this be effective? Why or why not?
6.  **Date/Time:** The script uses file modification time (`mtime`). What are `atime` and `ctime`, and in what scenarios might they be more appropriate?
7.  **Timezones:** The script uses `datetime.now()`, which is timezone-naive. How could this cause problems, and how would you make the script timezone-aware using the `datetime.timezone` module?
8.  **Permissions:** What potential issues related to file permissions might this script encounter when running, and how would you handle them in the code?
9.  **Atomicity:** The process of compressing and then moving is not atomic. How could you make the operation safer to prevent having a compressed file in the source directory if the move fails?
10. **Configuration:** Hardcoding `days_old=14` is inflexible. How would you modify the script to accept this value from a command-line argument using `argparse`?
11. **Logging:** Instead of `print()`, how would you integrate Python's `logging` module to provide different levels of output (e.g., DEBUG, INFO, ERROR) and log to a file?
12. **Symlinks:** How does your script currently handle symbolic links? Would it archive the link or the file it points to? How would you change this behavior?
13. **Recursion:** The current script only scans the top-level directory. How would you modify it to scan all subdirectories recursively using `os.walk()`?
14. **Testing:** How would you write a unit test for this script using `pytest`? What would you need to mock?
15. **Resource Management:** What happens if the script is killed halfway through compressing a large file? How might you handle cleanup of partial files?
16. **Alternative Compression:** How would you modify the script to support other compression formats like `bzip2` or `zip` based on a user's choice?
17. **Dry Run:** How would you add a `--dry-run` flag that prints the actions the script *would* take without actually modifying any files?
18. **Process Management:** In the `subprocess.run` call, what is the purpose of `check=True`? What would happen if you removed it?
19. **Scalability:** Imagine this script needs to run on a server that is constantly writing new log files. What potential race conditions could occur?
20. **Idempotency:** Is the script idempotent? If you run it twice in a row, what happens? How could you ensure it doesn't try to re-archive already archived files?

#### Follow-up Questions for Scenario 2 (Check Website Health)

1.  **HTTP Methods:** The script uses `requests.get()`. What is the difference between GET, POST, and HEAD requests? When would you use a HEAD request for a health check?
2.  **Status Codes:** The script only checks for `200`. What do status codes in the `3xx` (e.g., 301, 302), `4xx` (e.g., 401, 403), and `5xx` (e.g., 500, 503) ranges signify? How would you modify the script to treat `3xx` redirects as "UP"?
3.  **Concurrency:** The script checks URLs sequentially. This is slow. How would you rewrite it using `concurrent.futures.ThreadPoolExecutor` to check 20 URLs at a time?
4.  **Asyncio:** For checking thousands of URLs, threading has overhead. How would you rewrite the script using `asyncio` and the `aiohttp` library for maximum performance?
5.  **Request Sessions:** What is a `requests.Session` object? What performance benefit does it provide when checking multiple URLs on the same domain?
6.  **Headers:** How would you modify the script to send a custom `User-Agent` header with each request?
7.  **Content Verification:** A `200 OK` status doesn't guarantee the site is working correctly. How would you modify the script to also check that the response body contains a specific string (e.g., "Welcome")?
8.  **Configuration:** How would you modify the script to read not just URLs, but also the expected status code and content string from a JSON or YAML configuration file?
9.  **SSL/TLS:** What happens if a site has an invalid SSL certificate? What exception would `requests` raise, and how would you handle it? How can you disable certificate verification (and why is this dangerous)?
10. **Authentication:** How would you modify the script to check an endpoint that requires Basic Authentication?
11. **Timeouts:** The script has a single `timeout=5`. What is the difference between a connect timeout and a read timeout in the `requests` library?
12. **Retries:** How would you implement a retry mechanism so that if a request fails with a timeout or a `5xx` error, the script tries again up to 3 times with a delay between retries?
13. **Logging vs. Printing:** How would you refactor this to use the `logging` module, writing "UP" statuses to an INFO level and "DOWN" statuses to an ERROR level?
14. **Data Structures:** How would you refactor the function to return a list of dictionaries, with each dictionary containing the URL, status, status code, and response time, instead of just printing the results?
15. **Proxies:** How would you configure `requests` to send its traffic through an HTTP proxy?
16. **Streaming:** If you were checking a URL that returns a very large file, how could you use streaming requests to avoid loading the entire response into memory?
17. **Error Granularity:** The `RequestException` is very broad. What are some more specific exceptions in the `requests` library (like `ConnectionError`, `Timeout`, `HTTPError`) and how would you handle them differently?
18. **Command-Line Interface:** How would you use `argparse` to allow the user to specify the input file path and the timeout value from the command line?
19. **Output Format:** How would you add a feature to output the results as a JSON or CSV file instead of just printing to the console?
20. **Statefulness:** How would you modify the script to only send an alert if a site's status changes (e.g., it was "UP" on the last run but is "DOWN" now)? This would require storing the state between runs.

#### Follow-up Questions for Scenario 3 (Manage AWS S3)

1.  **Authentication:** The script relies on implicit credential discovery. What are the different ways `boto3` can find AWS credentials, and in what order does it search for them?
2.  **Error Handling:** The code catches `ClientError`. What kind of information is available in this exception object that can help you write more granular error handling (e.g., distinguishing "Access Denied" from "Not Found")?
3.  **Paginators:** The `list-files` command uses a paginator. Why is this necessary? What would happen if you used `list_objects_v2` directly on a bucket with 10,000 files?
4.  **Waiters:** If you were creating a new S3 bucket, the operation is not instantaneous. What is a `boto3` "waiter" and how would you use one to block your script until the bucket exists?
5.  **`argparse`:** What is the difference between `add_parser` and `add_subparsers`? Why are subparsers a good fit for this kind of tool?
6.  **Resource vs. Client:** `boto3` offers two levels of API: the client API and the resource API. What is the difference between them, and when might you prefer one over the other?
7.  **Streaming Uploads:** The `upload_file` method is a high-level managed transfer. How would you upload a very large file by streaming it from another source without saving it to disk first, using `upload_fileobj`?
8.  **Pre-signed URLs:** How would you add a new command, `share <bucket_name> <key>`, that generates a temporary, pre-signed URL to grant time-limited read access to a private S3 object?
9.  **Configuration:** How would you allow the user to specify the AWS region and profile from the command line?
10. **Testing:** How would you unit test the `list_all_buckets` function without making real API calls to AWS? (Hint: `botocore.stub.Stubber`).
11. **Performance:** For uploading thousands of small files, creating a new `upload_file` call for each is inefficient. How could you use `concurrent.futures` to parallelize the uploads?
12. **Multipart Uploads:** What is a multipart upload in S3? Does `upload_file` use it automatically? How does it improve reliability and performance for large files?
13. **Idempotency:** If you run the `upload` command twice with the same file, it will re-upload it. How could you modify the script to first check if an object with the same key and content already exists to avoid the re-upload? (Hint: ETag/MD5).
14. **Security:** What is the "Principle of Least Privilege" and how would you apply it when creating an IAM policy for the user/role running this script?
15. **Object Metadata:** How would you modify the `upload` command to include custom metadata (e.g., `x-amz-meta-author: "your-name"`) with the object?
16. **Listing with Prefixes:** How would you change the `list-files` command to function like a directory, allowing the user to list only files within a specific "folder" (prefix) in the bucket?
17. **Deletion:** How would you implement a `delete <bucket_name> <key>` command? What precautions should you take?
18. **Client Configuration:** How can you configure the `boto3` client to set custom timeouts or retry strategies?
19. **Exit Codes:** The script uses `sys.exit(1)` on error. Why is it important for automation scripts to use non-zero exit codes on failure?
20. **Packaging:** How would you package this script using `pyproject.toml` and a tool like `setuptools` so that it could be installed via `pip` and run as a system command?

---

### Part 8: Advanced DevOps Scripting Scenarios & Questions

This section presents more complex, multi-domain scripting challenges that mirror the work of a senior DevOps or SRE. These problems require combining knowledge of APIs, data structures, and cloud services to create robust automation.

#### Scenario 4 (Advanced): Find Failing Kubernetes Pods

*   **Problem:** Write a Python script that uses the `kubernetes` client library to scan a given namespace. The script should find all pods that are in a `CrashLoopBackOff` state and have a restart count greater than a specified threshold (e.g., 5 restarts). For each failing pod found, it should print the pod name, its restart count, and the reason for the last termination.

*   **Concepts Tested:** Interacting with the Kubernetes API via a client library, parsing complex nested object data, conditional logic, and formatted output.

*   **Solution:**
    ```python
    from kubernetes import client, config
    from kubernetes.client.rest import ApiException
    import sys

    def find_crashing_pods(namespace, restart_threshold=5):
        """
        Finds pods in a CrashLoopBackOff state with a high restart count.
        """
        try:
            # Load Kubernetes configuration (from ~/.kube/config or in-cluster)
            config.load_kube_config()
            api = client.CoreV1Api()
        except config.ConfigException:
            print("Error: Could not load Kubernetes configuration.")
            print("Ensure your kubeconfig file is correctly set up.")
            sys.exit(1)

        print(f"--- Scanning namespace '{namespace}' for pods with > {restart_threshold} restarts ---")
        
        try:
            pod_list = api.list_namespaced_pod(namespace, watch=False)
        except ApiException as e:
            if e.status == 404:
                print(f"Error: Namespace '{namespace}' not found.")
            else:
                print(f"Error listing pods in namespace '{namespace}': {e}")
            return

        found_crashing_pod = False
        for pod in pod_list.items:
            # We are interested in pods that are running but have crashing containers
            if pod.status.phase != 'Running' or not pod.status.container_statuses:
                continue

            for container_status in pod.status.container_statuses:
                restart_count = container_status.restart_count
                
                if restart_count > restart_threshold:
                    # Check if the container is in a waiting state with CrashLoopBackOff
                    if container_status.state.waiting and container_status.state.reason == 'CrashLoopBackOff':
                        found_crashing_pod = True
                        print(f"\n[FAIL] Pod: {pod.metadata.name}")
                        print(f"  Container: {container_status.name}")
                        print(f"  Restarts: {restart_count}")
                        
                        # Get the reason for the last termination
                        if container_status.last_state and container_status.last_state.terminated:
                            term_state = container_status.last_state.terminated
                            print(f"  Last Exit Code: {term_state.exit_code}")
                            print(f"  Last Reason: {term_state.reason}")
                            print(f"  Last Message: {term_state.message or 'N/A'}")
        
        if not found_crashing_pod:
            print("No pods found matching the crash criteria.")
        
        print("\n--- Scan Complete ---")

    if __name__ == "__main__":
        # To run this:
        # 1. pip install kubernetes
        # 2. Ensure your kubectl is configured to point to a cluster
        # 3. python your_script.py <namespace>
        if len(sys.argv) != 2:
            print("Usage: python your_script.py <namespace>")
            sys.exit(1)
        
        target_namespace = sys.argv[1]
        find_crashing_pods(target_namespace)
    ```

*   **Follow-up Questions:**
    1.  **Authentication:** The script uses `config.load_kube_config()`. What is the other primary method for authenticating to the Kubernetes API from within a pod, and how would you modify the script to use it?
    2.  **API Efficiency:** For a cluster with thousands of pods, `list_namespaced_pod` can be slow. How could you use `field_selector` or `label_selector` to ask the API server to pre-filter the results?
    3.  **Data Structure:** The `pod` object is deeply nested. What is a good way to explore its structure if you don't have the documentation handy?
    4.  **Alternative Tools:** How could you achieve a similar result using only `kubectl` with its `jsonpath` output format and a shell script? What are the advantages of using the Python client library over this?
    5.  **Watching for Changes:** The script is a one-time snapshot. How would you use the `watch` mechanism in the Kubernetes client to create a long-running script that reports on crashing pods in real-time?
    6.  **Error vs. Waiting:** The script checks for `state.waiting`. What is the difference between a container's `waiting`, `running`, and `terminated` states?
    7.  **Exit Codes:** What is the significance of exit code `137` vs. `1` in a terminated container?
    8.  **Output Format:** How would you modify the script to output the results as a JSON array, suitable for consumption by another tool?
    9.  **Contexts:** How would you modify the script to allow the user to specify which Kubernetes context to use from their kubeconfig file?
    10. **Resource Versions:** What is a `resourceVersion` in the Kubernetes API, and how can it be used for efficient polling?
    11. **Custom Resources (CRDs):** How would you adapt this script to check the status of a Custom Resource instead of a standard Pod?
    12. **Concurrency:** If you needed to scan 100 different namespaces, how would you parallelize the `list_namespaced_pod` calls?
    13. **Logging:** How could you extend the script to automatically fetch the logs from the previously terminated container when it finds a `CrashLoopBackOff`?
    14. **Robustness:** What happens if the `pod.status` or `container_statuses` are `None`? How can you write the accessors more defensively?
    15. **Threshold Configuration:** How would you use `argparse` to make the `restart_threshold` configurable from the command line?
    16. **API Verbs:** The script uses the `list` verb. What are other common Kubernetes API verbs?
    17. **Namespaces:** How would you modify the script to scan *all* namespaces in the cluster?
    18. **Owner References:** How could you extend the script to also report the name and kind of the object that "owns" the failing pod (e.g., the ReplicaSet or Deployment)?
    19. **Packaging:** How would you package this tool so it could be installed with `pip`?
    20. **Actionable Output:** Instead of just printing, how could you integrate this script with an alerting system (e.g., by sending a formatted message to a Slack webhook)?

#### Scenario 5 (Advanced): Automate Cloud Resource Cleanup

*   **Problem:** Write a Python script using `boto3` that scans all running EC2 instances in a specific AWS region. The script should identify any instance that is missing a required tag (e.g., a tag with the key `owner`). For each non-compliant instance, it should add a new tag, `cleanup-candidate`, with the value set to today's date.

*   **Concepts Tested:** Cloud SDK usage (`boto3`), resource filtering, resource tagging, and practical automation logic.

*   **Solution:**
    ```python
    import boto3
    from botocore.exceptions import ClientError
    import argparse
    from datetime import date

    def tag_untagged_instances(region, required_tag_key='owner', dry_run=False):
        """
        Finds running EC2 instances missing a specific tag and tags them for cleanup.
        """
        try:
            ec2 = boto3.client('ec2', region_name=region)
        except ClientError as e:
            print(f"Error creating EC2 client in region '{region}': {e}")
            return

        print(f"--- Scanning for running instances in region '{region}' missing tag '{required_tag_key}' ---")
        
        # Filter for instances that are in the 'running' state
        filters = [{'Name': 'instance-state-name', 'Values': ['running']}]
        
        try:
            paginator = ec2.get_paginator('describe_instances')
            pages = paginator.paginate(Filters=filters)
            
            instances_to_tag = []
            
            for page in pages:
                for reservation in page['Reservations']:
                    for instance in reservation['Instances']:
                        instance_id = instance['InstanceId']
                        tags = {tag['Key']: tag['Value'] for tag in instance.get('Tags', [])}
                        
                        if required_tag_key not in tags:
                            print(f"[NON-COMPLIANT] Instance ID: {instance_id} is missing the '{required_tag_key}' tag.")
                            instances_to_tag.append(instance_id)

            if not instances_to_tag:
                print("All running instances are compliant.")
                return

            print(f"\nFound {len(instances_to_tag)} non-compliant instances.")
            
            if dry_run:
                print("DRY RUN: Would tag the above instances with 'cleanup-candidate'. No action taken.")
            else:
                print("Tagging instances with 'cleanup-candidate'...")
                cleanup_tag = {'Key': 'cleanup-candidate', 'Value': str(date.today())}
                ec2.create_tags(Resources=instances_to_tag, Tags=[cleanup_tag])
                print("Tagging complete.")

        except ClientError as e:
            print(f"An AWS API error occurred: {e}")
        except Exception as e:
            print(f"An unexpected error occurred: {e}")

    if __name__ == "__main__":
        parser = argparse.ArgumentParser(description="Tag untagged EC2 instances for cleanup.")
        parser.add_argument('--region', type=str, required=True, help="The AWS region to scan.")
        parser.add_argument('--tag', type=str, default='owner', help="The required tag key to check for.")
        parser.add_argument('--dry-run', action='store_true', help="Run the script without making any changes.")
        
        args = parser.parse_args()
        
        tag_untagged_instances(args.region, args.tag, args.dry_run)
    ```

*   **Follow-up Questions:**
    1.  **Filtering:** The script filters instances on the client side. How could you use the `Filters` parameter in `describe_instances` to ask the AWS API to perform the filtering for you? Is it possible to filter for the *absence* of a tag key on the server side?
    2.  **Performance:** If you have thousands of instances, `describe_instances` can be slow. What is the `MaxResults` parameter, and how does it relate to pagination?
    3.  **Rate Limiting:** What happens if you run this script against an account with tens of thousands of instances and make too many API calls too quickly? What is "throttling" and how can you handle it with `boto3`?
    4.  **Cost:** What other resources, besides EC2 instances, are major contributors to cloud costs and could be targeted by similar cleanup scripts? (e.g., EBS volumes, S3 buckets, ELB).
    5.  **Alternative Action:** Instead of tagging, how would you modify the script to stop the instances? Why might tagging be a safer first step?
    6.  **IAM Policy:** Write an IAM policy that grants the minimum permissions required for this script to run.
    7.  **Multi-Account/Multi-Region:** How would you modify this script to run across multiple AWS accounts and all available EC2 regions?
    8.  **Lambda:** This script is run manually. How would you package it to run automatically as a daily AWS Lambda function triggered by EventBridge?
    9.  **State:** The script is stateless. How could you use a DynamoDB table to keep track of instances that have been tagged, and send a notification if an instance remains a cleanup candidate for more than 7 days?
    10. **Resource APIs:** You've used the `boto3` client. How would the code look different if you used the higher-level Resource API?
    11. **Tagging Atomicity:** The `create_tags` call can tag up to 1000 resources at once. What happens if the call fails halfway through? Is the operation atomic?
    12. **Data Classes:** The `instance` object is a raw dictionary. How could you use Python's `dataclasses` to create a more structured `Instance` object for easier use within your code?
    13. **Testing:** How would you test this script without running it against a real AWS account?
    14. **Exclusions:** How would you add a feature to exclude certain instances from being tagged, even if they are non-compliant (e.g., based on a list of instance IDs in a config file)?
    15. **Human-in-the-Loop:** How could you modify the script to generate a report (e.g., a CSV file) and wait for human approval before applying the tags?
    16. **Tag Value:** The script uses today's date as the tag value. What other information might be useful to include in the tag?
    17. **Cost Allocation Tags:** What are AWS Cost Allocation Tags, and why is the `owner` tag a common example?
    18. **Boto3 Sessions:** What is the difference between creating a default client (`boto3.client`) and creating a client from a `boto3.Session` object?
    19. **Error Reporting:** How could you integrate this with an alerting system to report how many non-compliant instances were found on each run?
    20. **Beyond EC2:** How would you adapt the core logic of this script to find unattached EBS volumes?

---

### Part 9: Real-World DevOps Automation Narratives

This section moves beyond interview questions to present case studies of how Python scripting is used to solve concrete, real-world problems that DevOps and SRE teams face every day. Each narrative describes a problem, the thought process, and the Python script that provides the solution.

#### Narrative 1: The Noisy Alert - Automating Alert Triage

*   **The Problem:** The SRE team at "Innovate Inc." uses Prometheus and Alertmanager for monitoring. They have an alert configured to fire if API latency exceeds 200ms. However, due to brief, transient network spikes, this alert fires frequently for just a minute or two before resolving itself. This creates constant noise in the team's Slack channel, causing "alert fatigue" and making it hard to spot real, persistent issues. The team decides they only want to be notified about this specific `HighLatency` alert if it has been in a "firing" state for more than 15 consecutive minutes.

*   **The Solution:** Build a Python-based webhook receiver using the Flask web framework. This script will act as an intelligent intermediary between Alertmanager and Slack.
    1.  Configure Alertmanager to send *all* its alerts to this new Python webhook instead of directly to Slack.
    2.  The Python application will receive the webhook payload from Alertmanager.
    3.  If the alert is **not** the noisy `HighLatency` alert, it will be forwarded to Slack immediately.
    4.  If the alert **is** the `HighLatency` alert, the script will check its status.
        *   If the status is "firing", the script will record the current timestamp for that alert in a simple in-memory dictionary.
        *   If the status is "resolved", the script will remove the alert's timestamp from the dictionary.
    5.  A separate background thread in the Python script runs every minute. It iterates through the dictionary of firing `HighLatency` alerts. If it finds an alert that has been firing for more than 15 minutes (by comparing its recorded start time to the current time), it then formats and sends a single, actionable notification to Slack.

*   **The Script:**
    ```python
    from flask import Flask, request, jsonify
    import requests
    import threading
    import time
    from datetime import datetime, timedelta

    app = Flask(__name__)

    # --- Configuration ---
    SLACK_WEBHOOK_URL = "https://hooks.slack.com/services/YOUR/SLACK/URL"
    NOISY_ALERT_NAME = "HighLatency"
    SUPPRESSION_MINUTES = 15

    # --- In-memory state store ---
    # Stores the time when a noisy alert started firing.
    # Format: { 'alert_fingerprint': 'start_time_iso' }
    firing_noisy_alerts = {}
    lock = threading.Lock()

    def alert_processor():
        """
        Background thread to check for persistently firing noisy alerts.
        """
        while True:
            time.sleep(60) # Check every minute
            
            with lock:
                now = datetime.utcnow()
                alerts_to_fire = []
                
                # Use a copy to avoid issues with modifying dict while iterating
                for fingerprint, start_time_str in list(firing_noisy_alerts.items()):
                    start_time = datetime.fromisoformat(start_time_str)
                    
                    if now - start_time > timedelta(minutes=SUPPRESSION_MINUTES):
                        print(f"Alert {fingerprint} has been firing for over {SUPPRESSION_MINUTES} minutes. Sending to Slack.")
                        # In a real app, you'd need the alert payload here.
                        # For simplicity, we'll just send a basic message.
                        message = {
                            "text": f":rotating_light: Persistent Alert: `{NOISY_ALERT_NAME}` is still firing!",
                            "username": "Alert Triage Bot"
                        }
                        requests.post(SLACK_WEBHOOK_URL, json=message)
                        
                        # Remove the alert so we don't send it again
                        alerts_to_fire.append(fingerprint)

                for fingerprint in alerts_to_fire:
                    del firing_noisy_alerts[fingerprint]


    @app.route('/webhook', methods=['POST'])
    def alert_webhook():
        data = request.json
        
        for alert in data.get('alerts', []):
            alert_name = alert.get('labels', {}).get('alertname')
            status = alert.get('status')
            fingerprint = alert.get('fingerprint')

            if alert_name == NOISY_ALERT_NAME:
                with lock:
                    if status == 'firing' and fingerprint not in firing_noisy_alerts:
                        print(f"Suppressing initial firing of '{NOISY_ALERT_NAME}' (Fingerprint: {fingerprint}).")
                        firing_noisy_alerts[fingerprint] = datetime.utcnow().isoformat()
                    elif status == 'resolved' and fingerprint in firing_noisy_alerts:
                        print(f"'{NOISY_ALERT_NAME}' (Fingerprint: {fingerprint}) has resolved. Removing from suppression.")
                        del firing_noisy_alerts[fingerprint]
            else:
                # For all other alerts, forward them immediately
                print(f"Forwarding alert '{alert_name}' directly to Slack.")
                # Re-format the alert for Slack (this is a simplified example)
                slack_message = {
                    "text": f"[{status.upper()}] {alert['annotations']['summary']}",
                    "username": "Alertmanager"
                }
                requests.post(SLACK_WEBHOOK_URL, json=slack_message)

        return jsonify(status='success')

    if __name__ == '__main__':
        # Start the background thread
        processor_thread = threading.Thread(target=alert_processor, daemon=True)
        processor_thread.start()
        
        # Run the Flask web server
        app.run(host='0.0.0.0', port=5001)
    ```

#### Narrative 2: The Certificate Expiry Panic - Proactive TLS Certificate Monitoring

*   **The Problem:** Innovate Inc. had a customer-facing service go down. After a frantic 30-minute investigation, the SRE team discovered the cause: the TLS certificate for `api.innovate.com` had expired overnight. The renewal process was manual and had been forgotten. The incident caused reputational damage. The Head of SRE mandates a new automated system: "I want a daily report of all public certificates expiring in the next 30 days. This must never happen again."

*   **The Solution:** A Python script that runs once a day via a cron job or a Kubernetes CronJob.
    1.  The script reads a list of hostnames from a simple text file.
    2.  For each hostname, it uses Python's built-in `ssl` and `socket` libraries to establish a TLS connection and retrieve the server's certificate.
    3.  It parses the certificate to extract the "not valid after" date.
    4.  It calculates the number of days remaining until the certificate expires.
    5.  If the certificate is expiring in 30 days or less, it formats a warning message.
    6.  At the end of the script, if any warnings were generated, it sends a single, consolidated report to a Slack channel.

*   **The Script:**
    ```python
    import ssl
    import socket
    from datetime import datetime
    import requests

    # --- Configuration ---
    HOSTS_FILE = "domains.txt"
    EXPIRY_THRESHOLD_DAYS = 30
    SLACK_WEBHOOK_URL = "https://hooks.slack.com/services/YOUR/SLACK/URL"

    def get_cert_expiry_date(hostname, port=443):
        """Connects to a host and returns the certificate's expiry date."""
        context = ssl.create_default_context()
        try:
            with socket.create_connection((hostname, port), timeout=5) as sock:
                with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                    cert = ssock.getpeercert()
                    expiry_date_str = cert['notAfter']
                    # Parse the date format e.g., 'Feb  4 12:00:00 2026 GMT'
                    return datetime.strptime(expiry_date_str, '%b %d %H:%M:%S %Y %Z')
        except Exception as e:
            print(f"Error checking {hostname}: {e}")
            return None

    def main():
        """Main function to check certificates and send report."""
        try:
            with open(HOSTS_FILE, 'r') as f:
                hostnames = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"Error: Host file '{HOSTS_FILE}' not found.")
            return

        warnings = []
        today = datetime.utcnow()

        for host in hostnames:
            expiry_date = get_cert_expiry_date(host)
            if expiry_date:
                days_left = (expiry_date - today).days
                if days_left < 0:
                    warnings.append(f":red_circle: *EXPIRED*: `{host}` certificate expired on {expiry_date.date()}.")
                elif days_left <= EXPIRY_THRESHOLD_DAYS:
                    warnings.append(f":warning: *EXPIRING SOON*: `{host}` certificate expires in *{days_left} days* (on {expiry_date.date()}).")
        
        if warnings:
            print("Found certificates that need attention. Sending Slack notification.")
            message_body = "\n".join(warnings)
            slack_message = {
                "text": "*TLS Certificate Expiry Report*",
                "attachments": [{
                    "color": "danger",
                    "text": message_body
                }]
            }
            requests.post(SLACK_WEBHOOK_URL, json=slack_message)
        else:
            print("All certificates are healthy.")

    if __name__ == "__main__":
        # Create a dummy domains.txt for testing
        with open("domains.txt", "w") as f:
            f.write("google.com\n")
            f.write("expired.badssl.com\n") # A site with an expired cert
            f.write("wrong.host.badssl.com\n") # A site with a host mismatch error
        
        main()
    ```

#### Narrative 3: The Manual Rollback - Automating Deployment Health Checks

*   **The Problem:** The platform team at Innovate Inc. has a CI/CD pipeline that deploys new versions of their main application. However, the team lacks confidence. After every deployment, a developer has to manually watch Grafana dashboards for 10 minutes to check for an increase in errors or latency. If they see a problem, they manually trigger a rollback. This process is slow, error-prone, and stressful.

*   **The Solution:** A "post-deployment verification" script written in Python.
    1.  The CI/CD pipeline is modified. After a deployment successfully finishes, the pipeline calls this Python script.
    2.  The script enters a loop that runs for 10 minutes.
    3.  Every minute, the script makes a query to the Prometheus API to get key application metrics for the new version:
        *   The HTTP error rate (percentage of 5xx status codes).
        *   The 95th percentile request latency.
    4.  The script compares these values against predefined thresholds stored in a configuration file (e.g., `error_rate_threshold: 1%`, `latency_threshold: 500ms`).
    5.  If a metric exceeds its threshold, the script immediately makes an API call back to the CI/CD system (e.g., GitLab, Jenkins, GitHub Actions) to trigger the "rollback" job and then exits with a non-zero status code, failing the pipeline.
    6.  If the script runs for the full 10 minutes and no thresholds are breached, it exits with a status code of 0, marking the deployment as successful.

*   **The Script:**
    ```python
    import requests
    import time
    import os
    import sys

    # --- Configuration ---
    PROMETHEUS_URL = "http://prometheus.example.com:9090"
    DEPLOYMENT_DURATION_MINS = 10
    CHECK_INTERVAL_SECS = 60
    
    # Thresholds
    ERROR_RATE_THRESHOLD = 0.01 # 1%
    LATENCY_THRESHOLD_MS = 500 # 500ms

    # CI/CD System Info (passed as environment variables)
    CI_API_URL = os.environ.get("CI_API_URL") # e.g., https://gitlab.com/api/v4/projects/123/jobs/456/retry
    CI_JOB_TOKEN = os.environ.get("CI_JOB_TOKEN")

    def query_prometheus(query):
        """Queries Prometheus and returns the result value."""
        try:
            response = requests.get(f"{PROMETHEUS_URL}/api/v1/query", params={'query': query})
            response.raise_for_status()
            data = response.json()
            if data['status'] == 'success' and data['data']['result']:
                return float(data['data']['result'][0]['value'][1])
            return None
        except Exception as e:
            print(f"Error querying Prometheus: {e}")
            return None

    def trigger_rollback():
        """Triggers the rollback job in the CI/CD system."""
        if not CI_API_URL or not CI_JOB_TOKEN:
            print("CI/CD environment variables not set. Cannot trigger rollback.")
            return
        
        print("!!! Threshold breached. Triggering rollback... !!!")
        headers = {"PRIVATE-TOKEN": CI_JOB_TOKEN} # Example for GitLab
        try:
            # This API call depends heavily on the CI/CD system
            response = requests.post(CI_API_URL, headers=headers)
            response.raise_for_status()
            print("Rollback job triggered successfully.")
        except requests.RequestException as e:
            print(f"Failed to trigger rollback job: {e}")

    def main():
        """Main verification loop."""
        end_time = time.time() + DEPLOYMENT_DURATION_MINS * 60
        
        while time.time() < end_time:
            print(f"--- Performing health check at {datetime.now().isoformat()} ---")
            
            # PromQL queries (these would be specific to your application)
            error_rate_query = 'sum(rate(http_requests_total{status=~"5.."}[1m])) / sum(rate(http_requests_total[1m]))'
            latency_query = 'histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[1m])) by (le))'

            error_rate = query_prometheus(error_rate_query)
            latency_ms = query_prometheus(latency_query) * 1000 # convert to ms

            if error_rate is not None:
                print(f"Current Error Rate: {error_rate:.2%}")
                if error_rate > ERROR_RATE_THRESHOLD:
                    print(f"Error rate {error_rate:.2%} exceeds threshold of {ERROR_RATE_THRESHOLD:.2%}.")
                    trigger_rollback()
                    sys.exit(1)

            if latency_ms is not None:
                print(f"Current p95 Latency: {latency_ms:.0f}ms")
                if latency_ms > LATENCY_THRESHOLD_MS:
                    print(f"Latency {latency_ms:.0f}ms exceeds threshold of {LATENCY_THRESHOLD_MS}ms.")
                    trigger_rollback()
                    sys.exit(1)

            print("Health check passed. Waiting for next interval...")
            time.sleep(CHECK_INTERVAL_SECS)

        print("\n--- Verification complete. Deployment is stable. ---")
        sys.exit(0)

    if __name__ == "__main__":
        # This script is meant to be run in a CI/CD environment
        # where environment variables are configured.
        print("Starting post-deployment verification...")
        # main() # In a real run, this would be uncommented.
        print("(Example run finished without actual checks)")
    ```

---

### Part 10: More Real-World Engineering Problems Solved

This section provides additional real-world narratives categorized by difficulty, showcasing how Python is a versatile tool for solving problems at every level of a DevOps/SRE career.

#### Narrative 4 (Basic): Server Reachability Checker

*   **The Problem:** A junior sysadmin is responsible for a fleet of 50 servers. Before every maintenance window, they must manually `ssh` into a bastion host and `ping` each server to ensure it's online. This is tedious and error-prone. They need a simple script to automate this check.

*   **The Solution:** A Python script that reads a list of server hostnames from a `servers.txt` file. For each server, it will execute the `ping` command using Python's `subprocess` module. It will check the exit code of the `ping` command to determine if the server is reachable and print a simple "UP" or "DOWN" status for each.

*   **The Script:**
    ```python
    import subprocess
    import platform
    import sys

    def check_server_reachability(filepath):
        """
        Reads server hostnames from a file and checks their reachability via ping.
        """
        try:
            with open(filepath, 'r') as f:
                hostnames = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"Error: The file '{filepath}' was not found.")
            sys.exit(1)

        if not hostnames:
            print("The server file is empty. No hosts to check.")
            return

        print("--- Server Reachability Check ---")
        
        # Determine the correct ping parameters based on the OS
        param = '-n' if platform.system().lower() == 'windows' else '-c'

        for host in hostnames:
            # Building the command: ['ping', '-c', '1', 'hostname']
            command = ['ping', param, '1', host]
            
            try:
                # Use subprocess.run to execute the command
                # stdout=subprocess.DEVNULL suppresses the command's output
                # stderr=subprocess.DEVNULL suppresses error messages (e.g., "host not found")
                result = subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                
                if result.returncode == 0:
                    print(f"[  UP  ] {host}")
                else:
                    print(f"[ DOWN ] {host}")

            except FileNotFoundError:
                print("Error: 'ping' command not found. Is it in your system's PATH?")
                sys.exit(1)
            except Exception as e:
                print(f"An error occurred while pinging {host}: {e}")

        print("--- Check Complete ---")

    if __name__ == "__main__":
        # Create a dummy servers.txt for testing
        with open("servers.txt", "w") as f:
            f.write("8.8.8.8\n")          # Google's DNS (should be up)
            f.write("1.1.1.1\n")          # Cloudflare's DNS (should be up)
            f.write("192.0.2.1\n")        # An example non-routable IP (should be down)

        check_server_reachability("servers.txt")
    ```

*   **Follow-up Questions:**
    1.  **Why `subprocess`?** Why is using `subprocess` a good choice for this task compared to a Python networking library?
    2.  **OS Compatibility:** Explain the purpose of the `platform.system()` check. What would happen if you ran this script on Windows without it?
    3.  **Output Suppression:** What does `stdout=subprocess.DEVNULL` do? Why is this useful?
    4.  **Return Codes:** What does a return code of `0` typically signify from a command-line tool?
    5.  **Alternative Check:** `ping` uses ICMP. What if ICMP is blocked by a firewall? What other method could you use to check if a web server is "up"? (e.g., checking if a specific TCP port is open).
    6.  **Concurrency:** The script checks servers one by one. How would you modify it to check 5 servers at a time?
    7.  **Timeouts:** The default `ping` timeout can be long. How would you modify the `ping` command to specify a timeout of 2 seconds?
    8.  **Error Handling:** What happens if the `servers.txt` file contains a malformed hostname? How would the script behave?
    9.  **Input:** How would you change the script to accept a single hostname from the command line instead of reading from a file?
    10. **Libraries:** Are there any third-party Python libraries designed to make running commands like this easier and more "Pythonic"? (e.g., `sh` or `plumbum`).

#### Narrative 5 (Intermediate): Log File Analysis - Finding Top Talkers

*   **The Problem:** An SRE is investigating a sudden increase in traffic to a web server. They suspect a single client or a bot might be responsible. They have a large NGINX `access.log` file and need to quickly find the top 10 IP addresses that have made the most requests.

*   **The Solution:** A Python script that can efficiently parse the log file.
    1.  The script will read the log file line by line to avoid loading the entire file into memory.
    2.  For each line, it will use a regular expression to extract the client IP address at the beginning of the line.
    3.  It will use Python's `collections.Counter` object, which is a highly optimized dictionary subclass, to store the IP addresses and their request counts.
    4.  After processing the entire file, it will use the `most_common()` method of the `Counter` to retrieve the top 10 IPs and their counts.
    5.  Finally, it will print a formatted report.

*   **The Script:**
    ```python
    import re
    from collections import Counter
    import argparse

    def find_top_ips(log_filepath, top_n=10):
        """
        Parses a log file to find the top N most frequent IP addresses.
        """
        # Regex to match an IP address at the start of a line
        # Handles IPv4. A more complex regex would be needed for IPv6.
        ip_regex = re.compile(r"^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}")
        
        ip_counts = Counter()

        try:
            with open(log_filepath, 'r') as f:
                print(f"Processing log file: {log_filepath}...")
                for line in f:
                    match = ip_regex.match(line)
                    if match:
                        ip = match.group(0)
                        ip_counts[ip] += 1
        except FileNotFoundError:
            print(f"Error: Log file not found at '{log_filepath}'")
            return
        except Exception as e:
            print(f"An error occurred while reading the file: {e}")
            return

        if not ip_counts:
            print("No IP addresses found in the log file.")
            return

        print(f"\n--- Top {top_n} IP Addresses ---")
        # The most_common() method makes this easy
        for ip, count in ip_counts.most_common(top_n):
            print(f"{ip:<16} {count} requests")

    if __name__ == "__main__":
        # Create a dummy access.log for testing
        dummy_log_data = """
        192.168.1.1 - - [05/Feb/2026:10:00:00 +0000] "GET /home HTTP/1.1" 200 1024
        8.8.8.8 - - [05/Feb/2026:10:00:01 +0000] "GET /api/data HTTP/1.1" 200 512
        192.168.1.1 - - [05/Feb/2026:10:00:02 +0000] "POST /login HTTP/1.1" 200 256
        1.1.1.1 - - [05/Feb/2026:10:00:03 +0000] "GET /home HTTP/1.1" 200 1024
        8.8.8.8 - - [05/Feb/2026:10:00:04 +0000] "GET /api/data HTTP/1.1" 200 512
        192.168.1.1 - - [05/Feb/2026:10:00:05 +0000] "GET /img/logo.png HTTP/1.1" 200 4096
        8.8.8.8 - - [05/Feb/2026:10:00:06 +0000] "GET /api/data HTTP/1.1" 503 128
        """
        with open("access.log", "w") as f:
            f.write(dummy_log_data.strip())

        parser = argparse.ArgumentParser(description="Find top IP addresses from a web server log.")
        parser.add_argument('log_file', help="Path to the log file.")
        parser.add_argument('-n', type=int, default=10, help="Number of top IPs to show.")
        args = parser.parse_args()

        find_top_ips(args.log_file, args.n)
    ```

*   **Follow-up Questions:**
    1.  **`collections.Counter`:** What is a `Counter`? How is it different from a standard dictionary? What is its performance characteristic for incrementing a count?
    2.  **Regular Expressions:** Explain the regex `^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}`. What does each part mean? How would you modify it to also match IPv6 addresses?
    3.  **Memory Efficiency:** Why is reading the file line-by-line with a `for` loop memory-efficient? What would be the problem with using `f.readlines()`?
    4.  **Alternative Parsing:** Instead of a regex, what other string methods could you use to extract the IP address, and what are their pros and cons? (e.g., `line.split()`).
    5.  **Extending the Analysis:** How would you modify the script to also report on the top 10 most requested URLs (e.g., `/home`, `/api/data`)?
    6.  **Filtering:** How would you change the script to only count requests that resulted in a `404 Not Found` error?
    7.  **Data Structures:** What if you needed to find the number of *unique* visitors per day? What data structure would you use to store this information?
    8.  **Standard Library:** Could you implement this without `collections.Counter`, using only a standard `dict`? What would the code look like?
    9.  **Performance:** For a 100 GB log file, this script might still be slow. How could you use the `multiprocessing` module to parallelize the processing of the file? What challenges would you face?
    10. **Log Rotation:** What happens if the log file is rotated while your script is running? How might you handle that?

#### Narrative 6 (Advanced): Automated Node Drain from a Load Balancer

*   **The Problem:** An SRE needs to perform kernel updates on a fleet of web servers that are part of an AWS Auto Scaling Group behind an Elastic Load Balancer (ELB). To do this safely, they must first "drain" the connections from an instance before shutting it down. The manual process involves going into the AWS Console, deregistering the instance, waiting and watching until the active connection count drops to zero, and then proceeding with maintenance. This is slow, manual, and risky.

*   **The Solution:** An automation script that gracefully drains and detaches an EC2 instance.
    1.  The script will take an EC2 instance ID as an argument.
    2.  It will use `boto3` to find the Target Group(s) the instance is registered with.
    3.  It will call `deregister_targets` for that instance. This tells the ELB to stop sending *new* traffic to the instance.
    4.  The script will then enter a polling loop. It will repeatedly call `describe_target_health` to monitor the state of the deregistered target.
    5.  It will wait until the target's state transitions from `draining` to `unused`. The `draining` state means the ELB is waiting for existing, in-flight requests to complete. `unused` means the connection drain is complete.
    6.  Once the state is `unused`, the script will print a success message, confirming that it is now safe to perform maintenance on the instance. It will also include a timeout to prevent the script from running forever if something goes wrong.

*   **The Script:**
    ```python
    import boto3
    from botocore.exceptions import ClientError
    import argparse
    import time

    def drain_instance_from_elb(instance_id, region, timeout_seconds=360):
        """
        Deregisters an instance from its target groups and waits for connections to drain.
        """
        try:
            elbv2 = boto3.client('elbv2', region_name=region)
            ec2 = boto3.client('ec2', region_name=region)
        except ClientError as e:
            print(f"Error creating AWS clients: {e}")
            return False

        # Find the target groups for the given instance
        try:
            # First, find the instance's VPC ID
            instance_info = ec2.describe_instances(InstanceIds=[instance_id])
            vpc_id = instance_info['Reservations'][0]['Instances'][0]['VpcId']
            
            # Now find target groups in that VPC
            all_tgs = elbv2.describe_target_groups()['TargetGroups']
            instance_tgs = []
            for tg in all_tgs:
                if tg['VpcId'] == vpc_id:
                    health = elbv2.describe_target_health(TargetGroupArn=tg['TargetGroupArn'])
                    for target in health['TargetHealthDescriptions']:
                        if target['Target']['Id'] == instance_id:
                            instance_tgs.append(tg['TargetGroupArn'])
                            break
            
            if not instance_tgs:
                print(f"Instance {instance_id} is not registered with any target groups. Nothing to do.")
                return True

        except ClientError as e:
            print(f"Error finding target groups for instance {instance_id}: {e}")
            return False

        print(f"Found instance {instance_id} in target groups: {instance_tgs}")

        # Deregister the instance from each target group
        for tg_arn in instance_tgs:
            print(f"Deregistering {instance_id} from {tg_arn}...")
            try:
                elbv2.deregister_targets(
                    TargetGroupArn=tg_arn,
                    Targets=[{'Id': instance_id}]
                )
            except ClientError as e:
                print(f"Failed to deregister instance: {e}")
                return False

        # Wait for the connection draining to complete
        print("Waiting for connections to drain...")
        start_time = time.time()
        
        while time.time() - start_time < timeout_seconds:
            all_drained = True
            for tg_arn in instance_tgs:
                try:
                    health = elbv2.describe_target_health(TargetGroupArn=tg_arn, Targets=[{'Id': instance_id}])
                    target_health = health['TargetHealthDescriptions'][0]
                    state = target_health['TargetHealth']['State']
                    print(f"  -> Current state for {instance_id} in {tg_arn.split('/')[-1]}: {state}")
                    
                    if state != 'unused':
                        all_drained = False
                        break # No need to check other TGs if one is not drained
                
                except (ClientError, IndexError):
                    # If describe_target_health fails or returns an empty list,
                    # it might mean the target is fully deregistered. We treat this as success.
                    print(f"  -> Instance {instance_id} no longer found in {tg_arn.split('/')[-1]}. Assuming drained.")
                    continue

            if all_drained:
                print(f"\nSuccess! Instance {instance_id} has been fully drained from all target groups.")
                return True
            
            time.sleep(15) # Wait 15 seconds before polling again
        
        print(f"\nError: Timed out after {timeout_seconds} seconds waiting for instance to drain.")
        return False

    if __name__ == "__main__":
        parser = argparse.ArgumentParser(description="Drain an EC2 instance from its ELB Target Groups.")
        parser.add_argument('--instance-id', required=True, help="The ID of the EC2 instance to drain.")
        parser.add_argument('--region', required=True, help="The AWS region of the instance.")
        parser.add_argument('--timeout', type=int, default=360, help="Timeout in seconds to wait for draining.")
        
        args = parser.parse_args()
        
        # In a real run, you would call the function.
        # For this example, we'll just print that we would call it.
        print("--- This is a simulation. In a real run, the following would be executed: ---")
        print(f"drain_instance_from_elb(instance_id='{args.instance_id}', region='{args.region}', timeout_seconds={args.timeout})")
        # drain_instance_from_elb(args.instance_id, args.region, args.timeout)
    ```

*   **Follow-up Questions:**
    1.  **Boto3 Waiters:** Polling with a `while` loop and `time.sleep()` is functional but verbose. What is a `boto3` "waiter"? Is there a pre-built waiter you could use for this scenario?
    2.  **Idempotency:** What happens if you run the script on an instance that has already been deregistered? Is the script idempotent?
    3.  **Auto Scaling Groups:** This script only detaches the instance from the ELB. What will the Auto Scaling Group (ASG) do in response to the instance becoming "unhealthy"? How would you properly suspend the ASG processes before running this script to prevent it from terminating the instance?
    4.  **Error States:** What other states can a target have besides `draining` and `unused`? (e.g., `initial`, `healthy`, `unhealthy`).
    5.  **Finding Target Groups:** The script's logic for finding the instance's target groups is complex. Is there a more direct way to query for this information, perhaps using a different API call?
    6.  **IAM Permissions:** What specific IAM actions are required for this script to function correctly? (`ec2:DescribeInstances`, `elasticloadbalancing:DescribeTargetGroups`, `elasticloadbalancing:DescribeTargetHealth`, `elasticloadbalancing:DeregisterTargets`).
    7.  **Timeouts:** Why is having a timeout crucial for a script like this? What are the risks of not having one?
    8.  **Return Values:** The function returns `True` or `False`. How does this make it more useful as part of a larger automation workflow compared to just printing?
    9.  **Alternative Triggers:** Instead of running this manually, how could you integrate it into a larger automation? For example, could you trigger it from a CI/CD pipeline before a deployment?
    10. **Human Safety:** How would you add a final confirmation prompt ("Are you sure you want to drain instance i-12345? [y/N]") to prevent accidental execution?
